{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Business Understanding\n",
        "\n",
        "## Introduction\n",
        "Traffic accidents are a critical public safety issue, causing injuries, fatalities and significant economic losses. Stakeholders such as traffic authorities and emergency services often face challenges in predicting and mitigating injury severity in crashes. Understanding the factors influencing injury outcomes can inform better policies, resource allocation and public safety to reduce injury severity and save lives.\n",
        "\n",
        "## Use Cases\n",
        "- Use the model to identify high-risk conditions (eg. weather, lighting, etc.) and implement measures like improved signage, speed limits or road design to reduce injury severity in traffic accidents.\n",
        "- Predict the severity of injuries based on crash conditions, enabling emergency services to prioritize resources and respond more effectively to severe accidents. \n",
        "\n",
        "## Value Proposition\n",
        "This project aims to develop a classification model that predicts injury severity in traffic crashes. By identifying key high-risk contributing to severe injuries, stakeholders can implement proactive measures to:\n",
        "    - Reduce injury severity in traffic accidents through ad-hoc interventions\n",
        "    - Enhance decision-making and resource allocation for emergency services\n",
        "    - Improve public safety and save lives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Business Objective\n",
        "- The task is to predict the severity of injuries based on the given features:\n",
        "    - Environment: The environment in which the accident occurred.\n",
        "        - POSTED_SPEED_LIMIT: The posted speed limit.\n",
        "        - WEAHTER_CONDITION: The weather condition.\n",
        "        - LIGHTING_CONDITION: The lighting condition.\n",
        "        - ROADWAY_SURFACE_COND: The roadway surface condition.\n",
        "        - ROAD_DEFECT: Whether or not the road was defective.\n",
        "        - TRAFFICWAY_TYPE: The type of trafficway.\n",
        "        - TRAFFIC_CONTROL_DEVICE: The traffic control device present at the location of the accident.\n",
        "    - Crash Dynamics: The dynamics of the crash.\n",
        "        - FIRST_CRASH_TYPE: The type of the first crash.\n",
        "        - TRAFFICWAY_TYPE: The type of trafficway.\n",
        "        - ALIGNMENT: The alignment of the road.\n",
        "        - LANE_CNT: The number of through lanes in either direction.\n",
        "        - CRASH_HOUR: The hour of the crash.\n",
        "        - CRASH_DAY_OF_WEEK: The day of the week of the crash.\n",
        "        - CRASH_MONTH: The month of the crash.\n",
        "    - Human Factors:\n",
        "        - PRIM_CONTRIBUTORY_CAUSE: The primary contributory cause of the accident.\n",
        "        - SEC_CONTRIBUTORY_CAUSE: The secondary contributory cause of the accident.\n",
        "        - HIT_AND_RUN_I: Whether or not the crash involved a hit and run.\n",
        "        - NOT_RIGHT_OF_WAY_I: Whether or not the crash involved a violation of the right of way.\n",
        "        - WORK_ZONE_I: Whether or not the crash occurred in a work zone.\n",
        "    - Location Factors:\n",
        "        - LATITUDE: The latitude of the location of the crash.\n",
        "        - LONGITUDE: The longitude of the location of the crash.\n",
        "        - BEAT_OF_OCCURRENCE: The police beat of occurrence.\n",
        "    - Target:\n",
        "        - MOST_SEVERE_INJURY: Multi-class classification target (eg. FATAL, INCAPACITATING INJURY, NONINCAPACITATING INJURY, REPORTED, NO INJURY).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Understanding\n",
        "\n",
        "## Introduction\n",
        "The dataset contains information about traffic accidents in Chicago. Stakeholders need reliable data-driven insights to mitigate injury severity and optimize their strategies. The dataset in this project is directly related to the task of predicting injury severity in traffic accidents.\n",
        "\n",
        "## Data Description\n",
        "- The dataset includes detailed records of traffic accidents covering various features such as environment, crash dynamics, human factors, location factors and target variable MOST_SEVERE_INJURY.\n",
        "\n",
        "## Data Quality\n",
        "- The dataset is very large with over 400,000 records and 49 features, providing a rich source of information for analysis.\n",
        "- The dataset comes from the City of Chicago's [open data portal](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if) and is updated daily making it a reliable source of information for stakeholders.\n",
        "\n",
        "## Data Relevance\n",
        "- Use data on crash conditions (eg. weather) to identify high-risk conditions take proative measures.\n",
        "- Predict injury severity to prioritize emergency services and allocate resources more effectively. \n",
        "\n",
        "## Conclusion\n",
        "The dataset is robust, relevant and continually updated, making it an indispensable resource for the task of predicting injury severity in traffic accidents. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation\n",
        "\n",
        "## Assembly\n",
        "- The source data is comprised of three CSV files:\n",
        "    - [Crash Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data)\n",
        "    - [Driver/Passenger Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d/about_data)\n",
        "    - [Vehicles Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3/about_data)\n",
        "- The data will be assembled into a single dataset by joining the three tables on the common key CRASH_RECORD_ID.\n",
        "\n",
        "## Cleaning\n",
        "- Irrelevant columns that do not contribute to the task will be dropped.\n",
        "- Missing values that will be imputed or dropped.\n",
        "\n",
        "## Transformation\n",
        "- Categorical features will be encoded using one-hot encoding.\n",
        "- Numerical features will be scaled using standard scaling.\n",
        "\n",
        "## Splitting\n",
        "- The dataset will be split into training and testing sets using a standard 80/20 split.\n",
        "- These sets will be saved to disk for future use as:\n",
        "    - [X_train](./data/X_train.csv)\n",
        "    - [X_test](./data/X_test.csv)\n",
        "    - [y_train](./data/y_train.csv)\n",
        "    - [y_test](./data/y_test.csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.feature_selection import f_classif\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from scipy.sparse import hstack\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zp/h7t69w7n1jvg_7vxjttlw77c0000gn/T/ipykernel_559/710280419.py:3: DtypeWarning: Columns (18,20,22,23,39,40,41,42,43,47,48,49,50,51,52,53,54,55,56,57,58,59,60,68,69,70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data_vehicles = pd.read_csv('./data/Traffic_Crashes_-_Vehicles_20250122.csv')\n",
            "/var/folders/zp/h7t69w7n1jvg_7vxjttlw77c0000gn/T/ipykernel_559/710280419.py:4: DtypeWarning: Columns (19,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data_people = pd.read_csv('./data/Traffic_Crashes_-_People_20250122.csv')\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "data = pd.read_csv('./data/Traffic_Crashes_-_Crashes_20250122.csv')\n",
        "data_vehicles = pd.read_csv('./data/Traffic_Crashes_-_Vehicles_20250122.csv')\n",
        "data_people = pd.read_csv('./data/Traffic_Crashes_-_People_20250122.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge data\n",
        "data = data.merge(data_vehicles, on='CRASH_RECORD_ID')\n",
        "data = data.merge(data_people, on='CRASH_RECORD_ID')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "- Handle missing values using imputation or dropping.\n",
        "- Drop columns with a high rate (> 50%) missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate null percentages\n",
        "null_percentage = data.isnull().mean() * 100\n",
        "\n",
        "# drop columns with more than 50% missing values\n",
        "columns_to_drop = null_percentage[null_percentage > 50].index\n",
        "data = data.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imputing Missing Values\n",
        "- Numerical features missing values will be imputed using the median.\n",
        "- Categorical features missing values will be imputed using the mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fill categorical columns with mode\n",
        "for column in data.select_dtypes(include='object').columns:\n",
        "    mode_value = data[column].mode()[0]\n",
        "    data[column] = data[column].fillna(mode_value)\n",
        "\n",
        "# fill numerical columns with median\n",
        "for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    median_value = data[column].median()\n",
        "    data[column] = data[column].fillna(median_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Analysis & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering & Outlier Removal\n",
        "- Group features values with low frequency into a single category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # replace low frequency categories with 'OTHER'\n",
        "# def group_rare_categories(data, col, threshold=2000):\n",
        "#     freq = data[col].value_counts()\n",
        "#     rare_labels = freq[freq < threshold].index\n",
        "#     return data[col].apply(lambda x: 'OTHER' if x in rare_labels else x)\n",
        "# \n",
        "# # Apply grouping to all categorical features\n",
        "# for feature in data.select_dtypes(include='object').columns:\n",
        "#     data[feature] = group_rare_categories(data, feature, threshold=200)\n",
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Selection for Categorical Features\n",
        "- Encode categorical features using one-hot encoding.\n",
        "- Encode target variable using label encoding.\n",
        "- Perform feature selection using chi-squared test.\n",
        "- Drop features outside p-value threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select categorical columns\n",
        "categorical = data.select_dtypes(include='object').columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save categorical to csv\n",
        "categorical_columns_df = pd.DataFrame(categorical, columns=['Categorical_Columns'])\n",
        "categorical_columns_df.to_csv('./data/categorical_columns.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_drop = [\n",
        "\n",
        "    # IDs\n",
        "    'CRASH_RECORD_ID',\n",
        "    'PERSON_ID', \n",
        "    'USDOT_NO', \n",
        "    'CCMC_NO', \n",
        "    'ILCC_NO', \n",
        "    'UN_NO', \n",
        "    'EMS_RUN_NO', \n",
        "    'IDOT_PERMIT_NO',\n",
        "\n",
        "    # Dates\n",
        "    'DATE_POLICE_NOTIFIED',\n",
        "    'CRASH_DATE_EST_I',\n",
        "    'CRASH_DATE_x',\n",
        "    'CRASH_DATE_x',\n",
        "    'CRASH_DATE_y',\n",
        "    'DATE_ACCIDENT',\n",
        "    'DATE_POLICE_NOTIFIED',\n",
        "    'CRASH_DATE_EST_I',\n",
        "\n",
        "    # Geographic\n",
        "    'CITY',\n",
        "    'STATE',\n",
        "    'ZIPCODE',\n",
        "    'LATITUDE',\n",
        "    'LONGITUDE',\n",
        "    'LOCATION', \n",
        "    'STREET_NAME',\n",
        "    'STREET_DIRECTION',\n",
        "    'CARRIER_STATE', \n",
        "    'CARRIER_CITY',\n",
        "\n",
        "    # # Misc.\n",
        "    'TOWED_BY',\n",
        "    'TOWED_TO',\n",
        "    'AREA_00_I',\n",
        "    'AREA_01_I',\n",
        "    'AREA_02_I',\n",
        "    'AREA_03_I',\n",
        "    'AREA_04_I',\n",
        "    'AREA_05_I',\n",
        "    'AREA_06_I',\n",
        "    'AREA_07_I',\n",
        "    'AREA_08_I',\n",
        "    'AREA_09_I',\n",
        "    'AREA_10_I',\n",
        "    'AREA_11_I',\n",
        "    'AREA_12_I',\n",
        "    'AREA_99_I',\n",
        "    'WORK_ZONE_TYPE',\n",
        "\n",
        "    # 'PHOTOS_TAKEN_I', \n",
        "    'STATEMENTS_TAKEN_I', \n",
        "    'DOORING_I',\n",
        "    'WIDE_LOAD_I',\n",
        "\n",
        "    # # Vehicle\n",
        "    'VEHICLE_ID',\n",
        "    'MAKE',\n",
        "    'MODEL',\n",
        "    'LIC_PLATE_STATE',\n",
        "    'TRAILER1_WIDTH', \n",
        "    'TRAILER2_WIDTH',\n",
        "    \n",
        "    # # hazzardous materials\n",
        "    'HAZMAT_PLACARDS_I', \n",
        "    'HAZMAT_NAME', \n",
        "    'HAZMAT_PRESENT_I', \n",
        "    'HAZMAT_REPORT_I', \n",
        "    'HAZMAT_REPORT_NO', \n",
        "    'HAZMAT_VIO_CAUSE_CRASH_I', \n",
        "    'HAZMAT_OUT_OF_SERVICE_I'\n",
        "\n",
        "    # # commercial vehicle\n",
        "    'COMMERCIAL_SRC', \n",
        "    'CARGO_BODY_TYPE', \n",
        "    'VEHICLE_CONFIG', \n",
        "    'GVWR', \n",
        "    'CARRIER_NAME', \n",
        "    'MCS_VIO_CAUSE_CRASH_I', \n",
        "    'MCS_REPORT_I', \n",
        "    'MCS_REPORT_NO', \n",
        "    'MCS_OUT_OF_SERVICE_I',\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data.drop(columns=categorical_drop, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select filtered categorical columns and save to csv\n",
        "categorical = data.select_dtypes(include='object').columns\n",
        "categorical_columns_df = pd.DataFrame(categorical, columns=['Categorical_Columns'])\n",
        "categorical_columns_df.to_csv('./checkpoint/filtered_categorical_columns.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encode Features\n",
        "- One hot encoding with dense matricies is too memory intensive.\n",
        "- Use a combination of frequency encoding and one-hot encoding with sparse matricies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "def limit_categories(column, threshold=100):\n",
        "    \"\"\"group rare categories into 'OTHER'.\"\"\"\n",
        "    freq = column.value_counts()\n",
        "    rare_categories = freq[freq < threshold].index\n",
        "    return column.apply(lambda x: 'OTHER' if x in rare_categories else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate categorical columns by cardinality\n",
        "low_cardinality_cols = [col for col in data.select_dtypes(include='object').columns if data[col].nunique() <= 50]\n",
        "high_cardinality_cols = [col for col in data.select_dtypes(include='object').columns if data[col].nunique() > 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "# one-hot encode low-cardinality columns\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "encoded_low_cardinality = onehot_encoder.fit_transform(data[low_cardinality_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get feature names for low-cardinality columns\n",
        "encoded_low_cardinality_feature_names = onehot_encoder.get_feature_names_out(low_cardinality_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "# frequency encode high-cardinality columns\n",
        "for col in high_cardinality_cols:\n",
        "    freq_map = data[col].value_counts(normalize=True)\n",
        "    data[col] = data[col].map(freq_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combine encoded low-cardinality data and frequency-encoded high-cardinality data\n",
        "# convert high-cardinality columns back to a DataFrame for alignment\n",
        "frequency_encoded_high_cardinality = data[high_cardinality_cols].to_numpy()\n",
        "combined_encoded_data = np.hstack([encoded_low_cardinality, frequency_encoded_high_cardinality])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate feature names for combined data\n",
        "combined_feature_names = np.concatenate([encoded_low_cardinality_feature_names, high_cardinality_cols])\n",
        "\n",
        "# perform Chi-Square test\n",
        "label_encoder = LabelEncoder()\n",
        "chi_scores, p_values = chi2(combined_encoded_data, label_encoder.fit_transform(data['MOST_SEVERE_INJURY']))\n",
        "\n",
        "# create a DataFrame for the Chi-Square results\n",
        "chi2_results = pd.DataFrame({\n",
        "    'Feature': combined_feature_names,\n",
        "    'Chi2 Score': chi_scores,\n",
        "    'P-Value': p_values\n",
        "})\n",
        "\n",
        "# select relevant features with p-value < 0.05\n",
        "# selected_categorical_features = chi2_results[chi2_results['P-Value'] < 0.05]['Feature'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the results to a CSV file\n",
        "chi2_results.to_csv('./checkpoint/chi2_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features to remove (15): {'MANEUVER_DIVERGING', 'VEHICLE_TYPE_FARM EQUIPMENT', 'VEHICLE_TYPE_SNOWMOBILE', 'VEHICLE_DEFECT_ENGINE/MOTOR', 'VEHICLE_DEFECT_CARGO', 'UNIT_TYPE_EQUESTRIAN', 'CRASH_DATE', 'VEHICLE_USE_CAMPER/RV - SINGLE UNIT', 'VEHICLE_DEFECT_RESTRAINT SYSTEM', 'VEHICLE_USE_HOUSE TRAILER', 'DEVICE_CONDITION_WORN REFLECTIVE MATERIAL', 'VEHICLE_DEFECT_SIGNALS', 'SAFETY_EQUIPMENT_SHOULD/LAP BELT USED IMPROPERLY', 'VEHICLE_DEFECT_TRAILER COUPLING', 'DRIVER_VISION_BLOWING MATERIALS'}\n"
          ]
        }
      ],
      "source": [
        "# Define the threshold for p-value\n",
        "p_value_threshold = 0.05\n",
        "\n",
        "# Filter features with p-value >= threshold\n",
        "insignificant_features = chi2_results[chi2_results['P-Value'] >= p_value_threshold]['Feature']\n",
        "\n",
        "# Optionally, filter out low chi-square score features if needed\n",
        "low_chi2_threshold = 10  # You can adjust this based on the context\n",
        "low_chi2_features = chi2_results[chi2_results['Chi2 Score'] < low_chi2_threshold]['Feature']\n",
        "\n",
        "# Combine features to drop\n",
        "features_to_drop = set(insignificant_features).union(set(low_chi2_features))\n",
        "\n",
        "# Print features to drop\n",
        "print(f\"Features to remove ({len(features_to_drop)}): {features_to_drop}\")\n",
        "\n",
        "# Drop these features from your dataset\n",
        "data = data.drop(columns=features_to_drop, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save selected features to csv\n",
        "final_categorical = data.select_dtypes(include='object').columns\n",
        "selected_categorical_features_df = pd.DataFrame(final_categorical, columns=['Selected_Categorical_Features'])\n",
        "selected_categorical_features_df.to_csv('./checkpoint/final_categorical_features.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Selection for Numerical Features\n",
        "- Perform feature selection using ANOVA test.\n",
        "- Drop features outside p-value threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "# identify numerical columns\n",
        "numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save numerical columns to csv\n",
        "numerical_columns_df = pd.DataFrame(numerical_columns, columns=['Numerical_Columns'])\n",
        "numerical_columns_df.to_csv('./checkpoint/numerical_columns.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "# collect numerical columns to drop\n",
        "numerical_drop = [\n",
        "    'STREET_NO',\n",
        "    'BEAT_OF_OCCURRENCE',\n",
        "    'CRASH_UNIT_ID',\n",
        "    'UNIT_NO',\n",
        "    'VEHICLE_ID_x',\n",
        "    'VEHICLE_ID_y'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop numerical columns\n",
        "data = data.drop(columns=numerical_drop, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rob/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [8] are constant.\n",
            "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
            "/Users/rob/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
            "  f = msb / msw\n"
          ]
        }
      ],
      "source": [
        "# perform ANOVA test\n",
        "f_scores, p_values = f_classif(data.select_dtypes(include=['float64', 'int64']), label_encoder.fit_transform(data['MOST_SEVERE_INJURY']))\n",
        "\n",
        "# create a DataFrame for ANOVA results\n",
        "anova_results = pd.DataFrame({\n",
        "    'Feature': data.select_dtypes(include=['float64', 'int64']).columns,\n",
        "    'F-Value': f_scores,\n",
        "    'P-Value': p_values\n",
        "})\n",
        "\n",
        "# filter significant features (p-value < 0.05)\n",
        "significant_features = anova_results[anova_results['P-Value'] < 0.05]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter the dataset to keep only significant numerical features\n",
        "#numerical = data[significant_features['Feature'].values]\n",
        "data = data[significant_features['Feature'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the filtered dataset if needed\n",
        "# numerical.to_csv('./checkpoint/filtered_numerical_features.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Features to Create Final Feature Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combine numerical and categorical features\n",
        "# data_encoded = pd.concat([numerical, data[final_categorical]], axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the one-hot encoded feature names\n",
        "# encoded_feature_names = onehot_encoder.get_feature_names_out(final_categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Analysis & Variance Inflation Factor\n",
        "- Check for correlation between features and target variable.\n",
        "- Drop features with low correlation with the target variable.\n",
        "- Drop features with high correlation with other features (r > 0.9).\n",
        "- Check for multicollinearity using the variance inflation factor (VIF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index(['TRAFFIC_CONTROL_DEVICE_DELINEATORS',\\n       'TRAFFIC_CONTROL_DEVICE_FLASHING CONTROL SIGNAL',\\n       'TRAFFIC_CONTROL_DEVICE_LANE USE MARKING',\\n       'TRAFFIC_CONTROL_DEVICE_NO CONTROLS',\\n       'TRAFFIC_CONTROL_DEVICE_NO PASSING', 'TRAFFIC_CONTROL_DEVICE_OTHER',\\n       'TRAFFIC_CONTROL_DEVICE_OTHER RAILROAD CROSSING',\\n       'TRAFFIC_CONTROL_DEVICE_OTHER REG. SIGN',\\n       'TRAFFIC_CONTROL_DEVICE_OTHER WARNING SIGN',\\n       'TRAFFIC_CONTROL_DEVICE_PEDESTRIAN CROSSING SIGN',\\n       ...\\n       'PHYSICAL_CONDITION_IMPAIRED - ALCOHOL AND DRUGS',\\n       'PHYSICAL_CONDITION_IMPAIRED - DRUGS', 'PHYSICAL_CONDITION_MEDICATED',\\n       'PHYSICAL_CONDITION_NORMAL', 'PHYSICAL_CONDITION_OTHER',\\n       'PHYSICAL_CONDITION_REMOVED BY EMS', 'PHYSICAL_CONDITION_UNKNOWN',\\n       'BAC_RESULT_TEST PERFORMED, RESULTS UNKNOWN', 'BAC_RESULT_TEST REFUSED',\\n       'BAC_RESULT_TEST TAKEN'],\\n      dtype='object', length=392)] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[178], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[0;32m---> 31\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_multicollinearity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_feature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReduced features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduced_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[178], line 20\u001b[0m, in \u001b[0;36mreduce_multicollinearity\u001b[0;34m(data, features, threshold)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mIteratively removes features with high VIF.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    list: Features with VIF below the threshold.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     vif_data \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_vif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     max_vif \u001b[38;5;241m=\u001b[39m vif_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_vif \u001b[38;5;241m>\u001b[39m threshold:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# Remove the feature with the highest VIF\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[178], line 4\u001b[0m, in \u001b[0;36mcalculate_vif\u001b[0;34m(data, features)\u001b[0m\n\u001b[1;32m      2\u001b[0m vif_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      3\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m----> 4\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [variance_inflation_factor(data[features]\u001b[38;5;241m.\u001b[39mvalues, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features))]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vif_data\n",
            "Cell \u001b[0;32mIn[178], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m vif_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      3\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m----> 4\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [variance_inflation_factor(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features))]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vif_data\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/pandas/core/indexes/base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/pandas/core/indexes/base.py:6248\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6250\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['TRAFFIC_CONTROL_DEVICE_DELINEATORS',\\n       'TRAFFIC_CONTROL_DEVICE_FLASHING CONTROL SIGNAL',\\n       'TRAFFIC_CONTROL_DEVICE_LANE USE MARKING',\\n       'TRAFFIC_CONTROL_DEVICE_NO CONTROLS',\\n       'TRAFFIC_CONTROL_DEVICE_NO PASSING', 'TRAFFIC_CONTROL_DEVICE_OTHER',\\n       'TRAFFIC_CONTROL_DEVICE_OTHER RAILROAD CROSSING',\\n       'TRAFFIC_CONTROL_DEVICE_OTHER REG. SIGN',\\n       'TRAFFIC_CONTROL_DEVICE_OTHER WARNING SIGN',\\n       'TRAFFIC_CONTROL_DEVICE_PEDESTRIAN CROSSING SIGN',\\n       ...\\n       'PHYSICAL_CONDITION_IMPAIRED - ALCOHOL AND DRUGS',\\n       'PHYSICAL_CONDITION_IMPAIRED - DRUGS', 'PHYSICAL_CONDITION_MEDICATED',\\n       'PHYSICAL_CONDITION_NORMAL', 'PHYSICAL_CONDITION_OTHER',\\n       'PHYSICAL_CONDITION_REMOVED BY EMS', 'PHYSICAL_CONDITION_UNKNOWN',\\n       'BAC_RESULT_TEST PERFORMED, RESULTS UNKNOWN', 'BAC_RESULT_TEST REFUSED',\\n       'BAC_RESULT_TEST TAKEN'],\\n      dtype='object', length=392)] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "# def calculate_vif(data, features):\n",
        "#     vif_data = pd.DataFrame()\n",
        "#     vif_data[\"Feature\"] = features\n",
        "#     vif_data[\"VIF\"] = [variance_inflation_factor(data[features].values, i) for i in range(len(features))]\n",
        "#     return vif_data\n",
        "# \n",
        "# def reduce_multicollinearity(data, features, threshold=10.0):\n",
        "#     \"\"\"\n",
        "#     Iteratively removes features with high VIF.\n",
        "# \n",
        "#     Parameters:\n",
        "#         data (DataFrame): Data containing features.\n",
        "#         features (list): List of numerical features to assess.\n",
        "#         threshold (float): VIF threshold to determine high multicollinearity.\n",
        "# \n",
        "#     Returns:\n",
        "#         list: Features with VIF below the threshold.\n",
        "#     \"\"\"\n",
        "#     while True:\n",
        "#         vif_data = calculate_vif(data, features)\n",
        "#         max_vif = vif_data['VIF'].max()\n",
        "#         if max_vif > threshold:\n",
        "#             # Remove the feature with the highest VIF\n",
        "#             feature_to_remove = vif_data.loc[vif_data['VIF'].idxmax(), 'Feature']\n",
        "#             features.remove(feature_to_remove)\n",
        "#             print(f\"Removed '{feature_to_remove}' with VIF={max_vif:.2f}\")\n",
        "#         else:\n",
        "#             break\n",
        "#     return features\n",
        "# \n",
        "# reduced_features = reduce_multicollinearity(data_encoded, encoded_feature_names, threshold=10.0)\n",
        "# print(f\"Reduced features: {reduced_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = data.drop(columns='MOST_SEVERE_INJURY')\n",
        "y = data['MOST_SEVERE_INJURY']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encode Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test-Train Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, \n",
        "    y_encoded, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_encoded\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Pipeline\n",
        "- Use a StandardScaler to scale the numerical features.\n",
        "- Use SMOTE to handle class imbalance.\n",
        "- Use Logistic Regression as the classification model.\n",
        "- Fit the model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the pipeline\n",
        "pipeline = Pipeline([\n",
        "    # standardize the features\n",
        "    ('scaler', StandardScaler()),  \n",
        "    # balance the class distribution\n",
        "    ('smote', SMOTE(random_state=42)),  \n",
        "    ('model', LogisticRegression(\n",
        "        random_state=42,\n",
        "        max_iter=800,\n",
        "        class_weight='balanced',\n",
        "        penalty='l2',\n",
        "        C=0.0001\n",
        "    ))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = pipeline.predict(X_test)\n",
        "# for roc-auc score\n",
        "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create and display the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create and display the classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cross validate the model\n",
        "cross_val_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f'Cross-validated scores: {cross_val_scores}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
