{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Business Understanding\n",
        "\n",
        "## Introduction\n",
        "Traffic accidents are a critical public safety issue, causing injuries, fatalities and significant economic losses. Stakeholders such as traffic authorities and emergency services often face challenges in predicting and mitigating injury severity in crashes. Understanding the factors influencing injury outcomes can inform better policies, resource allocation and public safety to reduce injury severity and save lives.\n",
        "\n",
        "## Use Cases\n",
        "- Use the model to identify high-risk conditions (eg. weather, lighting, etc.) and implement measures like improved signage, speed limits or road design to reduce injury severity in traffic accidents.\n",
        "- Predict the severity of injuries based on crash conditions, enabling emergency services to prioritize resources and respond more effectively to severe accidents. \n",
        "\n",
        "## Value Proposition\n",
        "This project aims to develop a classification model that predicts injury severity in traffic crashes. By identifying key high-risk contributing to severe injuries, stakeholders can implement proactive measures to:\n",
        "    - Reduce injury severity in traffic accidents through ad-hoc interventions\n",
        "    - Enhance decision-making and resource allocation for emergency services\n",
        "    - Improve public safety and save lives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Business Objective\n",
        "- The task is to predict the severity of injuries based on the given features:\n",
        "    - Environment: The environment in which the accident occurred.\n",
        "        - POSTED_SPEED_LIMIT: The posted speed limit.\n",
        "        - WEAHTER_CONDITION: The weather condition.\n",
        "        - LIGHTING_CONDITION: The lighting condition.\n",
        "        - ROADWAY_SURFACE_COND: The roadway surface condition.\n",
        "        - ROAD_DEFECT: Whether or not the road was defective.\n",
        "        - TRAFFICWAY_TYPE: The type of trafficway.\n",
        "        - TRAFFIC_CONTROL_DEVICE: The traffic control device present at the location of the accident.\n",
        "    - Crash Dynamics: The dynamics of the crash.\n",
        "        - FIRST_CRASH_TYPE: The type of the first crash.\n",
        "        - TRAFFICWAY_TYPE: The type of trafficway.\n",
        "        - ALIGNMENT: The alignment of the road.\n",
        "        - LANE_CNT: The number of through lanes in either direction.\n",
        "        - CRASH_HOUR: The hour of the crash.\n",
        "        - CRASH_DAY_OF_WEEK: The day of the week of the crash.\n",
        "        - CRASH_MONTH: The month of the crash.\n",
        "    - Human Factors:\n",
        "        - PRIM_CONTRIBUTORY_CAUSE: The primary contributory cause of the accident.\n",
        "        - SEC_CONTRIBUTORY_CAUSE: The secondary contributory cause of the accident.\n",
        "        - HIT_AND_RUN_I: Whether or not the crash involved a hit and run.\n",
        "        - NOT_RIGHT_OF_WAY_I: Whether or not the crash involved a violation of the right of way.\n",
        "        - WORK_ZONE_I: Whether or not the crash occurred in a work zone.\n",
        "    - Location Factors:\n",
        "        - LATITUDE: The latitude of the location of the crash.\n",
        "        - LONGITUDE: The longitude of the location of the crash.\n",
        "        - BEAT_OF_OCCURRENCE: The police beat of occurrence.\n",
        "    - Target:\n",
        "        - MOST_SEVERE_INJURY: Multi-class classification target (eg. FATAL, INCAPACITATING INJURY, NONINCAPACITATING INJURY, REPORTED, NO INJURY).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Understanding\n",
        "\n",
        "## Introduction\n",
        "The dataset contains information about traffic accidents in Chicago. Stakeholders need reliable data-driven insights to mitigate injury severity and optimize their strategies. The dataset in this project is directly related to the task of predicting injury severity in traffic accidents.\n",
        "\n",
        "## Data Description\n",
        "- The dataset includes detailed records of traffic accidents covering various features such as environment, crash dynamics, human factors, location factors and target variable MOST_SEVERE_INJURY.\n",
        "\n",
        "## Data Quality\n",
        "- The dataset is very large with over 400,000 records and 49 features, providing a rich source of information for analysis.\n",
        "- The dataset comes from the City of Chicago's [open data portal](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if) and is updated daily making it a reliable source of information for stakeholders.\n",
        "\n",
        "## Data Relevance\n",
        "- Use data on crash conditions (eg. weather) to identify high-risk conditions take proative measures.\n",
        "- Predict injury severity to prioritize emergency services and allocate resources more effectively. \n",
        "\n",
        "## Conclusion\n",
        "The dataset is robust, relevant and continually updated, making it an indispensable resource for the task of predicting injury severity in traffic accidents. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation\n",
        "\n",
        "## Assembly\n",
        "- The source data is comprised of three CSV files:\n",
        "    - [Crash Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data)\n",
        "    - [Driver/Passenger Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d/about_data)\n",
        "    - [Vehicles Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3/about_data)\n",
        "- The data will be assembled into a single dataset by joining the three tables on the common key CRASH_RECORD_ID.\n",
        "\n",
        "## Cleaning\n",
        "- Irrelevant columns that do not contribute to the task will be dropped.\n",
        "- Missing values that will be imputed or dropped.\n",
        "\n",
        "## Transformation\n",
        "- Categorical features will be encoded using one-hot encoding.\n",
        "- Numerical features will be scaled using standard scaling.\n",
        "\n",
        "## Splitting\n",
        "- The dataset will be split into training and testing sets using a standard 80/20 split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.feature_selection import f_classif\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from scipy.sparse import hstack\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import FunctionTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.stats import spearmanr\n",
        "import json\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from scipy.stats import chi2_contingency\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zp/h7t69w7n1jvg_7vxjttlw77c0000gn/T/ipykernel_78989/1241634393.py:3: DtypeWarning: Columns (18,20,39,40,41,43,47,48,49,52,54,57,58,60,70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data_vehicles = pd.read_csv('./data/Traffic_Crashes_-_Vehicles_20250127.csv')\n",
            "/var/folders/zp/h7t69w7n1jvg_7vxjttlw77c0000gn/T/ipykernel_78989/1241634393.py:4: DtypeWarning: Columns (19,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data_people= pd.read_csv('./data/Traffic_Crashes_-_People_20250127.csv')\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "data = pd.read_csv('./data/Traffic_Crashes_-_Crashes_20250127.csv')\n",
        "data_vehicles = pd.read_csv('./data/Traffic_Crashes_-_Vehicles_20250127.csv')\n",
        "data_people= pd.read_csv('./data/Traffic_Crashes_-_People_20250127.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge data\n",
        "data = data.merge(data_vehicles, on='CRASH_RECORD_ID')\n",
        "data = data.merge(data_people, on='CRASH_RECORD_ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assign target variable and drop it from the features\n",
        "target = data['MOST_SEVERE_INJURY']\n",
        "data.drop('MOST_SEVERE_INJURY', axis=1, inplace=True)\n",
        "\n",
        "# combine y and y_enc into a DataFrame\n",
        "label_encoder = LabelEncoder()\n",
        "target_enc = label_encoder.fit_transform(target)\n",
        "y_data = pd.DataFrame({'Original_Target': target, 'Encoded_Target': target_enc})\n",
        "\n",
        "# save to CSV\n",
        "y_data.to_csv('./checkpoint/target.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "y_data = pd.read_csv('./checkpoint/target.csv')\n",
        "\n",
        "# Access the columns\n",
        "y = y_data['Original_Target']  # Original target values\n",
        "y_enc = y_data['Encoded_Target']  # Encoded target values\n",
        "\n",
        "# If needed, convert y_enc back to integers (it might be loaded as floats)\n",
        "y_enc = y_enc.astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intial Clean Up\n",
        "- Drop features unlikely to influence injury severity.\n",
        "    - ids\n",
        "    - location\n",
        "    - date/time\n",
        "    - miscellaneous\n",
        "    - vehicle details\n",
        "    - hazmat details\n",
        "    - commerical vehicle details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_drop = [\n",
        "    # IDs\n",
        "    'CRASH_RECORD_ID', 'PERSON_ID', 'USDOT_NO', 'CCMC_NO', 'ILCC_NO', \n",
        "    'UN_NO', 'EMS_RUN_NO', 'IDOT_PERMIT_NO', 'UNIT_NO',\n",
        "\n",
        "    # Dates\n",
        "    'DATE_POLICE_NOTIFIED', 'CRASH_DATE_EST_I', 'CRASH_DATE_x', \n",
        "    'CRASH_DATE_y', 'CRASH_DATE',\n",
        "\n",
        "    # Geographic\n",
        "    'CITY', 'STATE', 'ZIPCODE', 'LATITUDE', 'LONGITUDE', 'LOCATION', \n",
        "    'STREET_NAME', 'STREET_DIRECTION', 'CARRIER_STATE', 'CARRIER_CITY', \n",
        "    'STREET_NO', 'BEAT_OF_OCCURRENCE', 'TRAVEL_DIRECTION',\n",
        "\n",
        "    # Miscellaneous\n",
        "    'TOWED_BY', 'TOWED_TO', 'AREA_00_I', 'AREA_01_I', 'AREA_02_I', \n",
        "    'AREA_03_I', 'AREA_04_I', 'AREA_05_I', 'AREA_06_I', 'AREA_07_I', \n",
        "    'AREA_08_I', 'AREA_09_I', 'AREA_10_I', 'AREA_11_I', 'AREA_12_I', \n",
        "    'AREA_99_I', 'WORK_ZONE_TYPE', 'PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I', \n",
        "    'DOORING_I', 'WIDE_LOAD_I', 'REPORT_TYPE', 'CRASH_TYPE',\n",
        "\n",
        "    # Vehicle\n",
        "    'VEHICLE_ID', 'MAKE', 'MODEL', 'LIC_PLATE_STATE', \n",
        "    'TRAILER1_WIDTH', 'TRAILER2_WIDTH',\n",
        "\n",
        "    # Hazardous Materials\n",
        "    'HAZMAT_PLACARDS_I', 'HAZMAT_NAME', 'HAZMAT_PRESENT_I', \n",
        "    'HAZMAT_REPORT_I', 'HAZMAT_REPORT_NO', 'HAZMAT_VIO_CAUSE_CRASH_I', \n",
        "    'HAZMAT_OUT_OF_SERVICE_I',\n",
        "\n",
        "    # Commercial Vehicle\n",
        "    'COMMERCIAL_SRC', 'CARGO_BODY_TYPE', 'VEHICLE_CONFIG', 'GVWR', \n",
        "    'CARRIER_NAME', 'MCS_VIO_CAUSE_CRASH_I', 'MCS_REPORT_I', \n",
        "    'MCS_REPORT_NO', 'MCS_OUT_OF_SERVICE_I',\n",
        "\n",
        "    # High/Inf VIF\n",
        "    'INJURIES_TOTAL', 'INJURIES_FATAL', 'INJURIES_INCAPACITATING', \n",
        "    'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT', \n",
        "    'CRASH_UNIT_ID', 'VEHICLE_ID_x', 'VEHICLE_ID_y',\n",
        "\n",
        "    # Related to Target\n",
        "    'INJURY_CLASSIFICATION', 'INJURIES_NO_INDICATION', 'INJURIES_UNKNOWN',\n",
        "\n",
        "    # Potential Drops\n",
        "    'VEHICLE_USE', 'BAC_RESULT', 'DRIVERS_LICENSE_STATE', \n",
        "\n",
        "    # Additional Drops\n",
        "    'CMRC_VEH_I', 'TRAVEL_DIRECTION', 'TRAILER1_LENGTH', 'TRAILER2_LENGTH', \n",
        "    'TOTAL_VEHICLE_LENGTH', 'AXLE_CNT', 'LOAD_TYPE', 'HAZMAT_CLASS', \n",
        "    'SEAT_NO', 'DRIVERS_LICENSE_CLASS', 'HOSPITAL', 'EMS_AGENCY', \n",
        "    'PEDPEDAL_ACTION', 'PEDPEDAL_VISIBILITY', 'PEDPEDAL_LOCATION', \n",
        "    'BAC_RESULT VALUE', 'CELL_PHONE_USE', 'CMV_ID',\n",
        "    'TOWED_I', 'FIRE_I'\n",
        "]\n",
        "\n",
        "# Drop columns\n",
        "data.drop(columns=categorical_drop, errors='ignore', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove Features with High Rate of Missing Values\n",
        "- Drop features with high rate of missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate null percentages\n",
        "null_percentage = data.isnull().mean() * 100\n",
        "\n",
        "# drop columns with more than 50% missing values\n",
        "columns_to_drop = null_percentage[null_percentage > 50].index\n",
        "data = data.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impute Missing Values\n",
        "- Median imputation for numerical features.\n",
        "- Mode imputation for categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grouping Rare Feature Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simplify all categorical features by grouping rare categories into a single category by proportion\n",
        "def simplify_all_categorical_features(df, rare_threshold=0.01, new_category='OTHER'):\n",
        "    for column in df.select_dtypes(include='object').columns:\n",
        "        total = len(df)\n",
        "        value_counts = df[column].value_counts()\n",
        "        rare_categories = value_counts[value_counts / total < rare_threshold].index\n",
        "        df[column] = df[column].replace(rare_categories, new_category)\n",
        "    return df\n",
        "\n",
        "data = simplify_all_categorical_features(data, rare_threshold=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fill categorical columns with mode\n",
        "for column in data.select_dtypes(include='object').columns:\n",
        "    mode_value = data[column].mode()[0]\n",
        "    data[column] = data[column].fillna(mode_value)\n",
        "\n",
        "# fill numerical columns with median\n",
        "for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    median_value = data[column].median()\n",
        "    data[column] = data[column].fillna(median_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simplify LIGHTING_CONDITION\n",
        "data['LIGHTING_CONDITION'] = data['LIGHTING_CONDITION'].replace({'DUSK': 'LOW LIGHT', 'DAWN': 'LOW LIGHT'})\n",
        "# data['Daylight'] = data['LIGHTING_CONDITION'].apply(lambda x: 'Daylight' if x == 'DAYLIGHT' else 'Other')\n",
        "\n",
        "# simplify contact point to FRONT, SIDE, REAR, OTHER\n",
        "data['Contact_Broad'] = data['FIRST_CONTACT_POINT'].apply(\n",
        "    lambda x: 'Front' if 'FRONT' in x.upper() else (\n",
        "        'Side' if 'SIDE' in x.upper() else (\n",
        "            'Rear' if 'REAR' in x.upper() else 'Other'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# create RUSH_HOUR feature\n",
        "data['CRASH_HOUR'] = pd.to_datetime(data['CRASH_HOUR'], format='%H').dt.hour\n",
        "data['RUSH_HOUR'] = data['CRASH_HOUR'].apply(\n",
        "    lambda x: 'Rush Hour' if 7 <= x <= 9 or 16 <= x <= 18 else 'Not Rush Hour'\n",
        ")\n",
        "\n",
        "# simplify vehicle types\n",
        "data['VEHICLE_TYPE'] = data['VEHICLE_TYPE'].replace({\n",
        "    'TRUCK - SINGLE UNIT': 'Truck',\n",
        "    'TRACTOR W/ SEMI-TRAILER': 'Truck',\n",
        "    'TRACTOR W/O SEMI-TRAILER': 'Truck',\n",
        "    'SINGLE UNIT TRUCK WITH TRAILER': 'Truck',\n",
        "    'OTHER VEHICLE WITH TRAILER': 'Truck',\n",
        "    'BUS OVER 15 PASS.': 'Bus',\n",
        "    'BUS UP TO 15 PASS.': 'Bus',\n",
        "    'MOTORCYCLE (OVER 150CC)': 'Motorcycle',\n",
        "    '3-WHEELED MOTORCYCLE (2 REAR WHEELS)': 'Motorcycle',\n",
        "    'AUTOCYCLE': 'Motorcycle',\n",
        "    'ALL-TERRAIN VEHICLE (ATV)': 'Motorcycle'\n",
        "})\n",
        "\n",
        "# group physical conditions\n",
        "data['PHYSICAL_CONDITION_GROUP'] = data['PHYSICAL_CONDITION'].replace({\n",
        "    'IMPAIRED - ALCOHOL': 'Impaired',\n",
        "    'IMPAIRED - DRUGS': 'Impaired',\n",
        "    'IMPAIRED - ALCOHOL AND DRUGS': 'Impaired',\n",
        "    'MEDICATED': 'Impaired',\n",
        "    'EMOTIONAL': 'Other',\n",
        "    'FATIGUED/ASLEEP': 'Other',\n",
        "    'ILLNESS/FAINTED': 'Other',\n",
        "    'HAD BEEN DRINKING': 'Impaired',\n",
        "    'NORMAL': 'Normal',\n",
        "    'UNKNOWN': 'Unknown'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSTED_SPEED_LIMIT          0\n",
            "TRAFFIC_CONTROL_DEVICE      0\n",
            "DEVICE_CONDITION            0\n",
            "WEATHER_CONDITION           0\n",
            "LIGHTING_CONDITION          0\n",
            "FIRST_CRASH_TYPE            0\n",
            "TRAFFICWAY_TYPE             0\n",
            "ALIGNMENT                   0\n",
            "ROADWAY_SURFACE_COND        0\n",
            "ROAD_DEFECT                 0\n",
            "DAMAGE                      0\n",
            "PRIM_CONTRIBUTORY_CAUSE     0\n",
            "SEC_CONTRIBUTORY_CAUSE      0\n",
            "NUM_UNITS                   0\n",
            "CRASH_HOUR                  0\n",
            "CRASH_DAY_OF_WEEK           0\n",
            "CRASH_MONTH                 0\n",
            "UNIT_TYPE                   0\n",
            "VEHICLE_YEAR                0\n",
            "VEHICLE_DEFECT              0\n",
            "VEHICLE_TYPE                0\n",
            "MANEUVER                    0\n",
            "OCCUPANT_CNT                0\n",
            "FIRST_CONTACT_POINT         0\n",
            "PERSON_TYPE                 0\n",
            "SEX                         0\n",
            "AGE                         0\n",
            "SAFETY_EQUIPMENT            0\n",
            "AIRBAG_DEPLOYED             0\n",
            "EJECTION                    0\n",
            "DRIVER_ACTION               0\n",
            "DRIVER_VISION               0\n",
            "PHYSICAL_CONDITION          0\n",
            "Contact_Broad               0\n",
            "RUSH_HOUR                   0\n",
            "PHYSICAL_CONDITION_GROUP    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# print features and their sum of missing values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# drop missing values\n",
        "data = data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checkpoint\n",
        "data.to_csv('./checkpoint/data_post_feat_eng.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "data = pd.read_csv('./checkpoint/data_post_feat_eng.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing & Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Significance Testing\n",
        "- Spearman correlation for numerical features.\n",
        "- Cramer's V for categorical features.\n",
        "- Chi-squared test for categorical target.\n",
        "- ANOVA for numerical target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate numeric and categorical features\n",
        "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# add the target variable back to the dataset for correlation\n",
        "data_with_target = data.copy()\n",
        "data_with_target['target'] = y_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pearson Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# compute correlation for numeric features\n",
        "pearson_corr = data_with_target[numeric_features + ['target']].corr(method='pearson')['target']\n",
        "\n",
        "# display correlations sorted by absolute value\n",
        "print(\"Numeric Feature Correlations:\")\n",
        "print(pearson_corr.abs().sort_values(ascending=False))\n",
        "\n",
        "# checkpoint results\n",
        "pearson_corr.to_csv('./checkpoint/pearson_corr.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Spearman Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Significant Features (Spearman Correlation):\n",
            "POSTED_SPEED_LIMIT: Correlation=0.0517, P-value=0.0000e+00\n",
            "NUM_UNITS: Correlation=0.1156, P-value=0.0000e+00\n",
            "CRASH_HOUR: Correlation=0.0089, P-value=9.5240e-74\n",
            "CRASH_DAY_OF_WEEK: Correlation=-0.0047, P-value=7.9192e-22\n",
            "CRASH_MONTH: Correlation=0.0089, P-value=3.0901e-74\n",
            "VEHICLE_YEAR: Correlation=-0.0035, P-value=3.4643e-13\n",
            "OCCUPANT_CNT: Correlation=0.0957, P-value=0.0000e+00\n",
            "AGE: Correlation=-0.0368, P-value=0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# add the encoded target variable to the dataset temporarily\n",
        "data_with_target = data.copy()\n",
        "data_with_target['target'] = y_enc\n",
        "\n",
        "significant_features = {}\n",
        "for feature in numeric_features:\n",
        "    # calculate Spearman correlation and p-value\n",
        "    corr, p_value = spearmanr(data_with_target[feature], y_enc)\n",
        "    significant_features[feature] = (corr, p_value)\n",
        "\n",
        "# filter features with p-value < 0.05\n",
        "significant_features = {k: v for k, v in significant_features.items() if v[1] < 0.05}\n",
        "\n",
        "# display significant features\n",
        "print(\"Significant Features (Spearman Correlation):\")\n",
        "for feature, (corr, p_value) in significant_features.items():\n",
        "    print(f\"{feature}: Correlation={corr:.4f}, P-value={p_value:.4e}\")\n",
        "\n",
        "# checkpoint results\n",
        "with open('./checkpoint/significant_features.json', 'w') as f:\n",
        "    json.dump(significant_features, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cramer's V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical Feature Correlations:\n",
            "FIRST_CRASH_TYPE            0.202951\n",
            "PERSON_TYPE                 0.156600\n",
            "AIRBAG_DEPLOYED             0.148654\n",
            "UNIT_TYPE                   0.142980\n",
            "PRIM_CONTRIBUTORY_CAUSE     0.116963\n",
            "DAMAGE                      0.107828\n",
            "PHYSICAL_CONDITION_GROUP    0.098137\n",
            "PHYSICAL_CONDITION          0.098137\n",
            "FIRST_CONTACT_POINT         0.085649\n",
            "TRAFFICWAY_TYPE             0.082182\n",
            "SAFETY_EQUIPMENT            0.081633\n",
            "MANEUVER                    0.074288\n",
            "DRIVER_ACTION               0.071252\n",
            "DEVICE_CONDITION            0.068263\n",
            "EJECTION                    0.067016\n",
            "Contact_Broad               0.063084\n",
            "TRAFFIC_CONTROL_DEVICE      0.061936\n",
            "SEX                         0.060986\n",
            "SEC_CONTRIBUTORY_CAUSE      0.058448\n",
            "DRIVER_VISION               0.051830\n",
            "LIGHTING_CONDITION          0.046018\n",
            "ROADWAY_SURFACE_COND        0.037424\n",
            "VEHICLE_TYPE                0.035304\n",
            "WEATHER_CONDITION           0.035051\n",
            "RUSH_HOUR                   0.030136\n",
            "ROAD_DEFECT                 0.025459\n",
            "ALIGNMENT                   0.016026\n",
            "VEHICLE_DEFECT              0.009138\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "data_with_target = data.copy()\n",
        "# note the y variable is unencoded\n",
        "data_with_target['target'] = y\n",
        "\n",
        "# calculate Cramers V for categorical features\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
        "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
        "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
        "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "# compute correlation for categorical features\n",
        "categorical_corr = pd.Series(index=categorical_features, dtype='float64')\n",
        "for feature in categorical_features:\n",
        "    categorical_corr[feature] = cramers_v(data_with_target[feature], data_with_target['target'])\n",
        "    \n",
        "# display correlations sorted by absolute value\n",
        "print(\"Categorical Feature Correlations:\")\n",
        "print(categorical_corr.abs().sort_values(ascending=False))\n",
        "\n",
        "# checkpoint results\n",
        "categorical_corr.to_csv('./checkpoint/categorical_corr.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chi-Square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Significant Features (Chi-Squared Test):\n",
            "TRAFFIC_CONTROL_DEVICE: Chi2=64648.2315, P-value=0.0000e+00\n",
            "DEVICE_CONDITION: Chi2=58895.3214, P-value=0.0000e+00\n",
            "WEATHER_CONDITION: Chi2=20719.7128, P-value=0.0000e+00\n",
            "LIGHTING_CONDITION: Chi2=35694.9952, P-value=0.0000e+00\n",
            "FIRST_CRASH_TYPE: Chi2=694019.9929, P-value=0.0000e+00\n",
            "TRAFFICWAY_TYPE: Chi2=113829.2689, P-value=0.0000e+00\n",
            "ALIGNMENT: Chi2=2171.5625, P-value=0.0000e+00\n",
            "ROADWAY_SURFACE_COND: Chi2=23613.1996, P-value=0.0000e+00\n",
            "ROAD_DEFECT: Chi2=5468.0663, P-value=0.0000e+00\n",
            "DAMAGE: Chi2=97955.1220, P-value=0.0000e+00\n",
            "PRIM_CONTRIBUTORY_CAUSE: Chi2=230548.7098, P-value=0.0000e+00\n",
            "SEC_CONTRIBUTORY_CAUSE: Chi2=57597.9448, P-value=0.0000e+00\n",
            "UNIT_TYPE: Chi2=258340.3797, P-value=0.0000e+00\n",
            "VEHICLE_DEFECT: Chi2=711.3812, P-value=2.5367e-148\n",
            "VEHICLE_TYPE: Chi2=21027.5064, P-value=0.0000e+00\n",
            "MANEUVER: Chi2=93022.8528, P-value=0.0000e+00\n",
            "FIRST_CONTACT_POINT: Chi2=123665.1882, P-value=0.0000e+00\n",
            "PERSON_TYPE: Chi2=309901.0956, P-value=0.0000e+00\n",
            "SEX: Chi2=31339.8510, P-value=0.0000e+00\n",
            "SAFETY_EQUIPMENT: Chi2=84220.9610, P-value=0.0000e+00\n",
            "AIRBAG_DEPLOYED: Chi2=372337.1070, P-value=0.0000e+00\n",
            "EJECTION: Chi2=37842.8585, P-value=0.0000e+00\n",
            "DRIVER_ACTION: Chi2=85578.3450, P-value=0.0000e+00\n",
            "DRIVER_VISION: Chi2=22638.1581, P-value=0.0000e+00\n",
            "PHYSICAL_CONDITION: Chi2=81141.4082, P-value=0.0000e+00\n",
            "Contact_Broad: Chi2=50300.3413, P-value=0.0000e+00\n",
            "RUSH_HOUR: Chi2=3829.2901, P-value=0.0000e+00\n",
            "PHYSICAL_CONDITION_GROUP: Chi2=81141.4082, P-value=0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "data_with_target = data.copy()\n",
        "data_with_target['target'] = y\n",
        "\n",
        "# chi-squared test for categorical features\n",
        "chi2_results = {}\n",
        "for feature in categorical_features:\n",
        "    # calculate chi-squared statistic and p-value\n",
        "    contingency_table = pd.crosstab(data_with_target[feature], data_with_target['target'])\n",
        "    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
        "    chi2_results[feature] = (chi2, p_value)\n",
        "    \n",
        "# filter features with p-value < 0.05\n",
        "chi2_results = {k: v for k, v in chi2_results.items() if v[1] < 0.05}\n",
        "\n",
        "# display significant features\n",
        "print(\"Significant Features (Chi-Squared Test):\")\n",
        "for feature, (chi2, p_value) in chi2_results.items():\n",
        "    print(f\"{feature}: Chi2={chi2:.4f}, P-value={p_value:.4e}\")\n",
        "\n",
        "# checkpoint results\n",
        "chi2_results_df = pd.DataFrame(chi2_results).T\n",
        "chi2_results_df.columns = ['Chi2', 'P-value']\n",
        "chi2_results_df.to_csv('./checkpoint/chi2_results.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ANOVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Significant Features (ANOVA Test):\n",
            "POSTED_SPEED_LIMIT: F-statistic=6693.1945, P-value=0.0000e+00\n",
            "NUM_UNITS: F-statistic=43090.0581, P-value=0.0000e+00\n",
            "CRASH_HOUR: F-statistic=40.5055, P-value=5.3934e-34\n",
            "CRASH_DAY_OF_WEEK: F-statistic=105.6114, P-value=3.9672e-90\n",
            "CRASH_MONTH: F-statistic=210.2451, P-value=1.0626e-180\n",
            "VEHICLE_YEAR: F-statistic=17.1931, P-value=4.1226e-14\n",
            "OCCUPANT_CNT: F-statistic=7827.2069, P-value=0.0000e+00\n",
            "AGE: F-statistic=1467.7508, P-value=0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# ANOVA test for numerical features\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "data_with_target = data.copy()\n",
        "data_with_target['target'] = y\n",
        "\n",
        "anova_results = {}\n",
        "\n",
        "for feature in numeric_features:\n",
        "    # calculate ANOVA F-statistic and p-value\n",
        "    groups = [group[1] for group in data_with_target.groupby('target')[feature]]\n",
        "    f_statistic, p_value = f_oneway(*groups)\n",
        "    anova_results[feature] = (f_statistic, p_value)\n",
        "\n",
        "# filter features with p-value < 0.05\n",
        "anova_results = {k: v for k, v in anova_results.items() if v[1] < 0.05}\n",
        "\n",
        "# display significant features\n",
        "print(\"Significant Features (ANOVA Test):\")\n",
        "for feature, (f_statistic, p_value) in anova_results.items():\n",
        "    print(f\"{feature}: F-statistic={f_statistic:.4f}, P-value={p_value:.4e}\")\n",
        "\n",
        "# checkpoint results\n",
        "anova_results_df = pd.DataFrame(anova_results).T\n",
        "anova_results_df.columns = ['F-statistic', 'P-value']\n",
        "anova_results_df.to_csv('./checkpoint/anova_results.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Drop Features with Low Significance\n",
        "- Drop features with p-value > 0.05."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop low relevance features based on anova, chi2, spearman, and cramers_v\n",
        "data.drop(columns=[\n",
        "    'VEHICLE_DEFECT', \n",
        "    'ALIGNMENT', \n",
        "    'Divided_Trafficway',\n",
        "    'VEHICLE_YEAR',\n",
        "    'Adverse_Weather',\n",
        "    'CRASH_DAY_BINARY',\n",
        "    'TRAFFICWAY_TYPE',\n",
        "    'DRIVER_ACTION',\n",
        "    'MANEUVER',\n",
        "    'ROADWAY_SURFACE_COND',\n",
        "    'LIGHTING_CONDITION',\n",
        "    'SEC_CAUSE_GROUP',\n",
        "    'ROAD_DEFECT',\n",
        "    'CRASH_MONTH',\n",
        "    'CRASH_HOUR',\n",
        "    'CRASH_DAY_OF_WEEK',\n",
        "    'WEATHER_CONDITION',\n",
        "    'OLD_VEHICLE',\n",
        "    'DRIVER_VISION'\n",
        "    \n",
        "    \n",
        "    ], errors='ignore', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features after dropping: 22\n"
          ]
        }
      ],
      "source": [
        "# print the number of features before dropping\n",
        "print(\"Number of features after dropping:\", data.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checkpoint data\n",
        "data.to_csv('./checkpoint/data_post_feat_sel.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Train Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "X = pd.read_csv('./checkpoint/data_post_feat_sel.csv')\n",
        "# y_enc = pd.read_csv('./checkpoint/target.csv')['Encoded_Target']\n",
        "y = pd.read_csv('./checkpoint/target.csv')['Original_Target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Values:\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "# check if there are any missing values\n",
        "missing_values = X.isnull().sum()\n",
        "print(\"Missing Values:\")\n",
        "print(missing_values[missing_values > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "# align indices of X and y, drop any rows with NaNs in either, and perform a train-test split\n",
        "def train_test_split_wrapper(data, y):\n",
        "    # align indices of data and y\n",
        "    data, y = data.align(y, join=\"inner\", axis=0)\n",
        "\n",
        "    # drop rows with NaNs in either the features or the target\n",
        "    combined = pd.concat([data, y], axis=1)\n",
        "    combined = combined.dropna()\n",
        "\n",
        "    # split the cleaned data and target\n",
        "    data_cleaned = combined.iloc[:, :-1]  # All columns except the last (features)\n",
        "    y_cleaned = combined.iloc[:, -1]  # Last column (target)\n",
        "\n",
        "    # train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data_cleaned, y_cleaned, test_size=0.2, random_state=42, stratify=y_cleaned\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perform train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split_wrapper(data, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "- One-hot encoding for categorical features.\n",
        "- Standard scaling for numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(df, scaler=None, encoder=None, train=True):\n",
        "    # identify numeric & categorical features\n",
        "    numeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # scale numeric features\n",
        "    if train:\n",
        "        scaler = StandardScaler()\n",
        "        df_numeric_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_features]), \n",
        "                                         columns=numeric_features, index=df.index)\n",
        "    else:\n",
        "        if scaler is None:\n",
        "            raise ValueError(\"Scaler cannot be None when train=False\")\n",
        "        df_numeric_scaled = pd.DataFrame(scaler.transform(df[numeric_features]), \n",
        "                                         columns=numeric_features, index=df.index)\n",
        "\n",
        "    # encode categorical features\n",
        "    if train:\n",
        "        encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "        df_categorical_encoded = pd.DataFrame(encoder.fit_transform(df[categorical_features]), \n",
        "                                              columns=encoder.get_feature_names_out(categorical_features), \n",
        "                                              index=df.index)\n",
        "    else:\n",
        "        if encoder is None:\n",
        "            raise ValueError(\"Encoder cannot be None when train=False\")\n",
        "        df_categorical_encoded = pd.DataFrame(encoder.transform(df[categorical_features]), \n",
        "                                              columns=encoder.get_feature_names_out(categorical_features), \n",
        "                                              index=df.index)\n",
        "\n",
        "    # combine processed features\n",
        "    df_processed = pd.concat([df_numeric_scaled, df_categorical_encoded], axis=1)\n",
        "\n",
        "    return df_processed, scaler, encoder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensure scaler & encoder are passed for test data\n",
        "X_train_processed, scaler, encoder = preprocess_data(X_train, train=True)\n",
        "X_test_processed, _, _ = preprocess_data(X_test, scaler=scaler, encoder=encoder, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Model\n",
        "- Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[99], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[1;32m      3\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1296\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1296\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    451\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[1;32m    452\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    453\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    454\u001b[0m ]\n\u001b[0;32m--> 455\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    470\u001b[0m     solver,\n\u001b[1;32m    471\u001b[0m     opt_res,\n\u001b[1;32m    472\u001b[0m     max_iter,\n\u001b[1;32m    473\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    475\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_optimize.py:78\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/scipy/optimize/_optimize.py:72\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 72\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/micromamba/envs/learn-env/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:300\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    298\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((n_classes, n_dof), dtype\u001b[38;5;241m=\u001b[39mweights\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# grad_pointwise.shape = (n_samples, n_classes)\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m grad[:, :n_features] \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_pointwise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m \u001b[38;5;241m+\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m weights\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[1;32m    302\u001b[0m     grad[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m grad_pointwise\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train baseline model\n",
        "model = LogisticRegression(\n",
        "    class_weight='balanced',\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train_processed, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Value Counts:\n",
            "Original_Target\n",
            "NO INDICATION OF INJURY     3401601\n",
            "NONINCAPACITATING INJURY     448135\n",
            "REPORTED, NOT EVIDENT        257391\n",
            "INCAPACITATING INJURY         97964\n",
            "FATAL                          7036\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# print target value counts\n",
        "print(\"Target Value Counts:\")\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X, y, X_test, y_test, output_path='./checkpoint/evaluation_metrics.json', cv_folds=5):\n",
        "    # classification report\n",
        "    print(\"🔹 Classification Report:\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "\n",
        "    # AUC-ROC\n",
        "    roc_auc = roc_auc_score(pd.get_dummies(y_test).values, y_pred_proba, multi_class=\"ovr\")\n",
        "    print(f\"🔹 AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "    # confusion matrix\n",
        "    print(\"🔹 Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    # cross-validation\n",
        "    print(\"\\n🔹 Running Cross-Validation...\")\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "    print(f\"🔹 Cross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "    # save evaluation metrics\n",
        "    evaluation_metrics = {\n",
        "        'Classification Report': classification_report(y_test, y_pred, output_dict=True),\n",
        "        'AUC-ROC': roc_auc,\n",
        "        'Confusion Matrix': cm.tolist(),\n",
        "        'Cross-Validation Accuracy': {\n",
        "            'mean': cv_scores.mean(),\n",
        "            'std_dev': cv_scores.std(),\n",
        "            'scores': cv_scores.tolist()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(evaluation_metrics, f, indent=4)\n",
        "\n",
        "    return evaluation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate the baseline model\n",
        "evaluation_metrics = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_baseline.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Tree\n",
        "- Use a decision tree classifier to predict injury severity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "                   FATAL       0.53      0.64      0.58      1407\n",
            "   INCAPACITATING INJURY       0.47      0.55      0.51     19593\n",
            " NO INDICATION OF INJURY       0.92      0.89      0.90    680321\n",
            "NONINCAPACITATING INJURY       0.53      0.58      0.55     89627\n",
            "   REPORTED, NOT EVIDENT       0.35      0.44      0.39     51478\n",
            "\n",
            "                accuracy                           0.82    842426\n",
            "               macro avg       0.56      0.62      0.59    842426\n",
            "            weighted avg       0.83      0.82      0.82    842426\n",
            "\n",
            "AUC-ROC: 0.7670\n",
            "Confusion Matrix:\n",
            "[[   895     96    192    160     64]\n",
            " [   134  10799   4490   2993   1177]\n",
            " [   261   7099 602791  36396  33774]\n",
            " [   311   3657  27594  51737   6328]\n",
            " [    76   1344  21387   5941  22730]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Classification Report': {'FATAL': {'precision': 0.5336911150864639,\n",
              "   'recall': 0.6361051883439943,\n",
              "   'f1-score': 0.5804150453955902,\n",
              "   'support': 1407.0},\n",
              "  'INCAPACITATING INJURY': {'precision': 0.4696238312676669,\n",
              "   'recall': 0.5511662328382586,\n",
              "   'f1-score': 0.5071381609843149,\n",
              "   'support': 19593.0},\n",
              "  'NO INDICATION OF INJURY': {'precision': 0.918253221093938,\n",
              "   'recall': 0.886039090370575,\n",
              "   'f1-score': 0.9018585775467075,\n",
              "   'support': 680321.0},\n",
              "  'NONINCAPACITATING INJURY': {'precision': 0.5321258498153805,\n",
              "   'recall': 0.5772479275218405,\n",
              "   'f1-score': 0.5537692529996682,\n",
              "   'support': 89627.0},\n",
              "  'REPORTED, NOT EVIDENT': {'precision': 0.3547516114431976,\n",
              "   'recall': 0.4415478456816504,\n",
              "   'f1-score': 0.393419355955379,\n",
              "   'support': 51478.0},\n",
              "  'accuracy': 0.8178190131833538,\n",
              "  'macro avg': {'precision': 0.5616891257413295,\n",
              "   'recall': 0.6184212569512637,\n",
              "   'f1-score': 0.587320078576332,\n",
              "   'support': 842426.0},\n",
              "  'weighted avg': {'precision': 0.8316622940745882,\n",
              "   'recall': 0.8178190131833538,\n",
              "   'f1-score': 0.824038490902065,\n",
              "   'support': 842426.0}},\n",
              " 'AUC-ROC': 0.766997107087763,\n",
              " 'Confusion Matrix': [[895, 96, 192, 160, 64],\n",
              "  [134, 10799, 4490, 2993, 1177],\n",
              "  [261, 7099, 602791, 36396, 33774],\n",
              "  [311, 3657, 27594, 51737, 6328],\n",
              "  [76, 1344, 21387, 5941, 22730]]}"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train a decision tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# train the model\n",
        "model = DecisionTreeClassifier(\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# get evaluation metrics\n",
        "evaluation_metrics = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_decision_tree.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Results\n",
        "- Decision Tree Classifier performs better than the baseline model.\n",
        "    - ROC-AUC: Logistic Regression is slightly better at distinguishing between classes.\n",
        "    - Weighted Average F1 Score: Decision Tree Classifier is better at predicting injury severity.\n",
        "    - Accuracy: Decision Tree is overall better at predicting injury severity.\n",
        "- Logistic Regression suffers from class imbalance and is not able to predict injury severity effectively.\n",
        "\n",
        "Logistic Regression:\n",
        "- It heavily predicts the majority class (\"No Indication of Injury\"), even for rare classes.\n",
        "    - Fatal injuries (FATAL): Poor recall (69%), likely due to underrepresentation.\n",
        "    - Struggles to separate \"Injury\" classes (e.g., Incapacitating vs. Non-incapacitating).\n",
        "Decision Tree:\n",
        "- Better balance in predictions, but slightly worse AUC-ROC.\n",
        "    - Higher recall for all injury types, meaning it captures more true injuries.\n",
        "    - Slightly more false positives in injury-related classes, which could be tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Improvement\n",
        "- Investigate sampling techniques to improve model performance.\n",
        "    - Under-sampling\n",
        "    - Class weights\n",
        "\n",
        "Note that SMOTE will not be considered as it is computationally expensive and may not be suitable for large datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Undersampling \n",
        "- Use undersampling to balance the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train_processed, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the model\n",
        "model.fit(X_train_undersampled, y_train_undersampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get evaluation metrics\n",
        "evaluation_metrics = evaluate_model(model, X_train_undersampled, y_train_undersampled, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_decision_tree_undersampled.json')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
