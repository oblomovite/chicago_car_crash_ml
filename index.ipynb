{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Business Understanding\n",
        "\n",
        "## Introduction\n",
        "Stakeholders such as traffic authorities and emergency services often face challenges in predicting and mitigating fatal/severe injuries in traffic crashes. \n",
        "By identifying key factors contributing to fatal/severe injury, stakeholders can gain useful insights to implement proactive measures to reduce injury severity and save lives.\n",
        "\n",
        "## Use Cases\n",
        "- Use the model to identify high-risk conditions and implement measures like improved signage, speed limits or road design to reduce injury severity in traffic accidents.\n",
        "- Predict whether an incident is fatal or not based on crash conditions, enhancing decision-making and resource allocation for emergency services by prioritizing high-risk incidents.\n",
        "\n",
        "## Value Proposition\n",
        "This project aims to develop a classification model that predicts injury severity in traffic crashes. By identifying key high-risk contributing to severe injuries, stakeholders can implement proactive measures to:\n",
        "    - Enhance decision-making and resource allocation for emergency services\n",
        "    - Improve public safety and save lives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Understanding\n",
        "\n",
        "## Introduction\n",
        "The dataset contains information about traffic accidents in Chicago. Stakeholders need reliable data-driven insights to mitigate injury severity and optimize their strategies. The dataset in this project is directly related to the task of predicting injury severity in traffic accidents.\n",
        "\n",
        "## Data Description\n",
        "- The dataset includes detailed records of traffic accidents covering various features such as environment, crash dynamics, human factors, location factors and target variable MOST_SEVERE_INJURY.\n",
        "\n",
        "## Data Quality\n",
        "- The dataset is very large with over 650,000 records and 49 features, providing a rich source of information for analysis.\n",
        "- The dataset comes from the City of Chicago's [open data portal](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if) and is updated daily making it a reliable source of information for stakeholders.\n",
        "\n",
        "## Data Relevance\n",
        "- Use data on crash conditions (eg. weather) to identify high-risk conditions take proative measures.\n",
        "- Predict injury severity to prioritize emergency services and allocate resources more effectively. \n",
        "\n",
        "## Conclusion\n",
        "The dataset is robust, relevant and continually updated, making it an indispensable resource for the task of predicting injury severity in traffic accidents. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation\n",
        "\n",
        "## Assembly\n",
        "- The source data is comprised of three CSV files:\n",
        "    - [Crash Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data)\n",
        "    - [Driver Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d/about_data)\n",
        "    - [Vehicles Data](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3/about_data)\n",
        "- The data will be assembled into a single dataset by joining the three tables on the common key CRASH_RECORD_ID.\n",
        "\n",
        "## Cleaning\n",
        "- Irrelevant columns that do not contribute to the task will be dropped.\n",
        "- Missing values that will be imputed or dropped.\n",
        "\n",
        "## Transformation\n",
        "- Categorical features will be encoded using one-hot encoding.\n",
        "- Numerical features will be scaled using standard scaling.\n",
        "\n",
        "## Splitting\n",
        "- The dataset will be split into training and testing sets using a standard 80/20 split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.feature_selection import f_classif\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from scipy.sparse import hstack\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import FunctionTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.stats import spearmanr\n",
        "import json\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from scipy.stats import chi2_contingency\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zp/h7t69w7n1jvg_7vxjttlw77c0000gn/T/ipykernel_88313/1241634393.py:3: DtypeWarning: Columns (18,20,39,40,41,43,47,48,49,52,54,57,58,60,70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data_vehicles = pd.read_csv('./data/Traffic_Crashes_-_Vehicles_20250127.csv')\n",
            "/var/folders/zp/h7t69w7n1jvg_7vxjttlw77c0000gn/T/ipykernel_88313/1241634393.py:4: DtypeWarning: Columns (19,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data_people= pd.read_csv('./data/Traffic_Crashes_-_People_20250127.csv')\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "data = pd.read_csv('./data/Traffic_Crashes_-_Crashes_20250127.csv')\n",
        "data_vehicles = pd.read_csv('./data/Traffic_Crashes_-_Vehicles_20250127.csv')\n",
        "data_people= pd.read_csv('./data/Traffic_Crashes_-_People_20250127.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge data\n",
        "data = data.merge(data_vehicles, on='CRASH_RECORD_ID')\n",
        "data = data.merge(data_people, on='CRASH_RECORD_ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assign target variable - MOST_SEVERE_INJURY and convert to binary 0: NON-FATAL, 1: FATAL\n",
        "data['MOST_SEVERE_INJURY'] = data['MOST_SEVERE_INJURY'].apply(lambda x: 0 if x != 'FATAL' else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intial Clean Up\n",
        "- Drop features unlikely to influence injury severity.\n",
        "    - ids\n",
        "    - location\n",
        "    - date/time\n",
        "    - miscellaneous\n",
        "    - vehicle details\n",
        "    - hazmat details\n",
        "    - commerical vehicle details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_drop = [\n",
        "    # IDs\n",
        "    'CRASH_RECORD_ID', 'PERSON_ID', 'USDOT_NO', 'CCMC_NO', 'ILCC_NO', 'VEHICLE_ID_x', 'VEHICLE_ID_y', 'EMS_RUN_NO', 'IDOT_PERMIT_NO', 'UNIT_NO',\n",
        "\n",
        "    # Dates\n",
        "    'DATE_POLICE_NOTIFIED', 'CRASH_DATE_EST_I', 'CRASH_DATE_x', 'CRASH_DATE_y', 'CRASH_DATE',\n",
        "\n",
        "    # Geographic\n",
        "    'CITY', 'STATE', 'ZIPCODE', 'LATITUDE', 'LONGITUDE', 'LOCATION', 'STREET_NAME', 'STREET_DIRECTION', 'CARRIER_STATE', 'CARRIER_CITY', 'STREET_NO', 'BEAT_OF_OCCURRENCE', 'TRAVEL_DIRECTION',\n",
        "\n",
        "    # Miscellaneous\n",
        "    'TOWED_BY', 'TOWED_TO', 'AREA_00_I', 'AREA_01_I', 'AREA_02_I', 'AREA_03_I', 'AREA_04_I', 'AREA_05_I', 'AREA_06_I', 'AREA_07_I', 'AREA_08_I', 'AREA_09_I', 'AREA_10_I', 'AREA_11_I', 'AREA_12_I', 'AREA_99_I', 'WORK_ZONE_TYPE', 'PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I', 'DOORING_I', 'WIDE_LOAD_I', 'REPORT_TYPE', 'CRASH_TYPE',\n",
        "\n",
        "    # Vehicle\n",
        "    'VEHICLE_ID', 'MAKE', 'MODEL', 'LIC_PLATE_STATE', 'TRAILER1_WIDTH', 'TRAILER2_WIDTH',\n",
        "\n",
        "    # Hazardous Materials\n",
        "    'HAZMAT_PLACARDS_I', 'HAZMAT_NAME', 'HAZMAT_PRESENT_I', 'HAZMAT_REPORT_I', 'HAZMAT_REPORT_NO', 'HAZMAT_VIO_CAUSE_CRASH_I', 'HAZMAT_OUT_OF_SERVICE_I',\n",
        "\n",
        "    # Commercial Vehicle\n",
        "    'COMMERCIAL_SRC', 'CARGO_BODY_TYPE', 'VEHICLE_CONFIG', 'GVWR', 'CARRIER_NAME', 'MCS_VIO_CAUSE_CRASH_I', 'MCS_REPORT_I', 'MCS_REPORT_NO', 'MCS_OUT_OF_SERVICE_I',\n",
        "\n",
        "    # High/Inf VIF\n",
        "    'INJURIES_TOTAL', 'INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT', 'CRASH_UNIT_ID', \n",
        "\n",
        "    # Related to Target\n",
        "    'INJURY_CLASSIFICATION', 'INJURIES_NO_INDICATION', 'INJURIES_UNKNOWN',\n",
        "\n",
        "    # Potential Drops\n",
        "    'VEHICLE_USE', 'BAC_RESULT', 'DRIVERS_LICENSE_STATE', \n",
        "\n",
        "    # Additional Drops\n",
        "    'CMRC_VEH_I', 'TRAVEL_DIRECTION', 'TRAILER1_LENGTH', 'TRAILER2_LENGTH', 'TOTAL_VEHICLE_LENGTH', 'AXLE_CNT', 'LOAD_TYPE', 'HAZMAT_CLASS', 'SEAT_NO', 'DRIVERS_LICENSE_CLASS', 'HOSPITAL', 'EMS_AGENCY', 'PEDPEDAL_ACTION', 'PEDPEDAL_VISIBILITY', 'PEDPEDAL_LOCATION', 'BAC_RESULT VALUE', 'CELL_PHONE_USE', 'CMV_ID', 'TOWED_I', 'FIRE_I', 'DAMAGE'\n",
        "]\n",
        "\n",
        "# Drop columns\n",
        "data.drop(columns=categorical_drop, errors='ignore', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove Features with High Rate of Missing Values\n",
        "- Drop features with high rate of missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate null percentages\n",
        "null_percentage = data.isnull().mean() * 100\n",
        "\n",
        "# drop columns with more than 50% missing values\n",
        "columns_to_drop = null_percentage[null_percentage > 50].index\n",
        "data = data.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impute Missing Values\n",
        "- Median imputation for numerical features.\n",
        "- Mode imputation for categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fill categorical columns with mode\n",
        "for column in data.select_dtypes(include='object').columns:\n",
        "    mode_value = data[column].mode()[0]\n",
        "    data[column] = data[column].fillna(mode_value)\n",
        "\n",
        "# fill numerical columns with median\n",
        "for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    median_value = data[column].median()\n",
        "    data[column] = data[column].fillna(median_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simplify contact point to FRONT, SIDE, REAR, OTHER\n",
        "data['FIRST_CONTACT_POINT'] = data['FIRST_CONTACT_POINT'].apply(\n",
        "    lambda x: 'Front' if 'FRONT' in x.upper() else (\n",
        "        'Side' if 'SIDE' in x.upper() else (\n",
        "            'Rear' if 'REAR' in x.upper() else 'Other'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# create RUSH_HOUR feature\n",
        "data['CRASH_HOUR'] = pd.to_datetime(data['CRASH_HOUR'], format='%H').dt.hour\n",
        "data['RUSH_HOUR'] = data['CRASH_HOUR'].apply(\n",
        "    lambda x: 1 if 7 <= x <= 9 or 16 <= x <= 18 else 0\n",
        ")\n",
        "\n",
        "# create DAYLIGHT feature from LIGHTING_CONDITION and CRASH_HOUR\n",
        "data['LIGHTING_CONDITION'] = data['LIGHTING_CONDITION'].replace({'DARKNESS, LIGHTED ROAD': 'DARKNESS', 'DARKNESS, ROADWAY NOT LIGHTED': 'DARKNESS'})\n",
        "data['DAYLIGHT'] = data['LIGHTING_CONDITION'].apply(\n",
        "    lambda x: 1 if x == 'DAYLIGHT' else 0\n",
        ")\n",
        "    \n",
        "data.drop(columns=['LIGHTING_CONDITION', 'CRASH_HOUR'], inplace=True)\n",
        "\n",
        "# convert VEHICLE_YEAR to OLD, NEW\n",
        "data['VEHICLE_YEAR'] = data['VEHICLE_YEAR'].apply(\n",
        "    lambda x: 'Old' if x < 2010 else 'New'\n",
        ")\n",
        "\n",
        "# bin AGE into groups - child, young, middle, old\n",
        "data['AGE'] = data['AGE'].apply(\n",
        "    lambda x: 'Child' if x < 18 else (\n",
        "        'Young' if 18 <= x < 30 else (\n",
        "            'Middle' if 30 <= x < 60 else 'Old'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# bin OCCUPANT_CNT into groups - solo, small, medium, large\n",
        "data['OCCUPANT_CNT'] = data['OCCUPANT_CNT'].apply(\n",
        "    lambda x: 'Solo' if x == 1 else (\n",
        "        'Small' if 2 <= x <= 4 else (\n",
        "            'Medium' if 5 <= x <= 7 else 'Large'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# POSTED_SPEED_LIMIT          46\n",
        "\n",
        "# simplify vehicle types\n",
        "data['VEHICLE_TYPE'] = data['VEHICLE_TYPE'].replace({\n",
        "    'TRUCK - SINGLE UNIT': 'Truck',\n",
        "    'TRACTOR W/ SEMI-TRAILER': 'Truck',\n",
        "    'TRACTOR W/O SEMI-TRAILER': 'Truck',\n",
        "    'SINGLE UNIT TRUCK WITH TRAILER': 'Truck',\n",
        "    'OTHER VEHICLE WITH TRAILER': 'Truck',\n",
        "    'BUS OVER 15 PASS.': 'Bus',\n",
        "    'BUS UP TO 15 PASS.': 'Bus',\n",
        "    'MOTORCYCLE (OVER 150CC)': 'Motorcycle',\n",
        "    '3-WHEELED MOTORCYCLE (2 REAR WHEELS)': 'Motorcycle',\n",
        "    'AUTOCYCLE': 'Motorcycle',\n",
        "    'ALL-TERRAIN VEHICLE (ATV)': 'Motorcycle'\n",
        "})\n",
        "\n",
        "# group physical conditions\n",
        "data['PHYSICAL_CONDITION'] = data['PHYSICAL_CONDITION'].replace({\n",
        "    'IMPAIRED - ALCOHOL': 'Impaired',\n",
        "    'IMPAIRED - DRUGS': 'Impaired',\n",
        "    'IMPAIRED - ALCOHOL AND DRUGS': 'Impaired',\n",
        "    'MEDICATED': 'Impaired',\n",
        "    'EMOTIONAL': 'Other',\n",
        "    'FATIGUED/ASLEEP': 'Other',\n",
        "    'ILLNESS/FAINTED': 'Other',\n",
        "    'HAD BEEN DRINKING': 'Impaired',\n",
        "    'NORMAL': 'Normal',\n",
        "    'UNKNOWN': 'Unknown'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grouping Rare Feature Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSTED_SPEED_LIMIT         46\n",
            "PRIM_CONTRIBUTORY_CAUSE    40\n",
            "SEC_CONTRIBUTORY_CAUSE     40\n",
            "MANEUVER                   28\n",
            "DRIVER_ACTION              20\n",
            "TRAFFICWAY_TYPE            20\n",
            "TRAFFIC_CONTROL_DEVICE     19\n",
            "SAFETY_EQUIPMENT           19\n",
            "FIRST_CRASH_TYPE           18\n",
            "VEHICLE_DEFECT             17\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# print columns with the most unique values\n",
        "unique_values = data.nunique().sort_values(ascending=False)\n",
        "print(unique_values.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simplify all categorical features by grouping rare categories into a single category by proportion\n",
        "def simplify_all_categorical_features(df, rare_threshold=0.01, new_category='OTHER'):\n",
        "    for column in df.select_dtypes(include='object').columns:\n",
        "        total = len(df)\n",
        "        value_counts = df[column].value_counts()\n",
        "        rare_categories = value_counts[value_counts / total < rare_threshold].index\n",
        "        df[column] = df[column].replace(rare_categories, new_category)\n",
        "    return df\n",
        "\n",
        "# data = simplify_all_categorical_features(data, rare_threshold=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checkpoint\n",
        "data.to_csv('./checkpoint/data_post_feat_eng.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "data = pd.read_csv('./checkpoint/data_post_feat_eng.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing & Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Significance Testing\n",
        "- Spearman correlation for numerical features.\n",
        "- Cramer's V for categorical features.\n",
        "- Chi-squared test for categorical target.\n",
        "- ANOVA for numerical target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate numeric and categorical features\n",
        "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Spearman Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Significant Features (Spearman Correlation):\n",
            "POSTED_SPEED_LIMIT: Correlation=0.0095, P-value=7.6187e-84\n",
            "NUM_UNITS: Correlation=0.0333, P-value=0.0000e+00\n",
            "MOST_SEVERE_INJURY: Correlation=1.0000, P-value=0.0000e+00\n",
            "CRASH_DAY_OF_WEEK: Correlation=-0.0021, P-value=1.2434e-05\n",
            "CRASH_MONTH: Correlation=0.0015, P-value=2.0181e-03\n",
            "RUSH_HOUR: Correlation=-0.0118, P-value=7.2680e-129\n",
            "DAYLIGHT: Correlation=-0.0218, P-value=0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "significant_features = {}\n",
        "for feature in numeric_features:\n",
        "    # calculate Spearman correlation and p-value\n",
        "    corr, p_value = spearmanr(data[feature], data['MOST_SEVERE_INJURY'])\n",
        "    significant_features[feature] = (corr, p_value)\n",
        "\n",
        "# filter features with p-value < 0.05\n",
        "significant_features = {k: v for k, v in significant_features.items() if v[1] < 0.05}\n",
        "\n",
        "# display significant features\n",
        "print(\"Significant Features (Spearman Correlation):\")\n",
        "for feature, (corr, p_value) in significant_features.items():\n",
        "    print(f\"{feature}: Correlation={corr:.4f}, P-value={p_value:.4e}\")\n",
        "\n",
        "# checkpoint\n",
        "significant_features_df = pd.DataFrame(significant_features).T\n",
        "significant_features_df.columns = ['Correlation', 'P-value']\n",
        "significant_features_df.to_csv('./checkpoint/spearman.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop features with p-value > 0.05\n",
        "# data.drop(columns=[feature for feature in numeric_features if feature not in significant_features], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ANOVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zp/h7t69w7n1jvg_7vxjttlw77c0000gn/T/ipykernel_88313/1670838748.py:9: ConstantInputWarning: Each of the input arrays is constant; the F statistic is not defined or infinite\n",
            "  f_statistic, p_value = f_oneway(*groups)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Significant Features (ANOVA Test):\n",
            "POSTED_SPEED_LIMIT: F-statistic=425.0732, P-value=1.9404e-94\n",
            "NUM_UNITS: F-statistic=12094.5171, P-value=0.0000e+00\n",
            "MOST_SEVERE_INJURY: F-statistic=inf, P-value=0.0000e+00\n",
            "CRASH_DAY_OF_WEEK: F-statistic=20.8532, P-value=4.9588e-06\n",
            "CRASH_MONTH: F-statistic=11.5371, P-value=6.8222e-04\n",
            "RUSH_HOUR: F-statistic=583.3168, P-value=7.2680e-129\n",
            "DAYLIGHT: F-statistic=2003.5118, P-value=0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# ANOVA test for numerical features\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "anova_results = {}\n",
        "\n",
        "for feature in numeric_features:\n",
        "    # calculate ANOVA F-statistic and p-value\n",
        "    groups = [group[1] for group in data.groupby('MOST_SEVERE_INJURY')[feature]]\n",
        "    f_statistic, p_value = f_oneway(*groups)\n",
        "    anova_results[feature] = (f_statistic, p_value)\n",
        "\n",
        "# filter features with p-value < 0.05\n",
        "anova_results = {k: v for k, v in anova_results.items() if v[1] < 0.05}\n",
        "\n",
        "# display significant features\n",
        "print(\"Significant Features (ANOVA Test):\")\n",
        "for feature, (f_statistic, p_value) in anova_results.items():\n",
        "    print(f\"{feature}: F-statistic={f_statistic:.4f}, P-value={p_value:.4e}\")\n",
        "\n",
        "# checkpoint results\n",
        "anova_results_df = pd.DataFrame(anova_results).T\n",
        "anova_results_df.columns = ['F-statistic', 'P-value']\n",
        "anova_results_df.to_csv('./checkpoint/anova.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cramer's V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical Feature Correlations:\n",
            "FIRST_CRASH_TYPE           0.069679\n",
            "PRIM_CONTRIBUTORY_CAUSE    0.060734\n",
            "AIRBAG_DEPLOYED            0.053426\n",
            "EJECTION                   0.051340\n",
            "UNIT_TYPE                  0.046917\n",
            "PERSON_TYPE                0.045784\n",
            "PHYSICAL_CONDITION         0.038977\n",
            "SEC_CONTRIBUTORY_CAUSE     0.038758\n",
            "SAFETY_EQUIPMENT           0.032656\n",
            "VEHICLE_TYPE               0.022371\n",
            "TRAFFICWAY_TYPE            0.019669\n",
            "DRIVER_ACTION              0.019166\n",
            "MANEUVER                   0.018044\n",
            "OCCUPANT_CNT               0.016459\n",
            "FIRST_CONTACT_POINT        0.014872\n",
            "TRAFFIC_CONTROL_DEVICE     0.013418\n",
            "WEATHER_CONDITION          0.012191\n",
            "DRIVER_VISION              0.010694\n",
            "DEVICE_CONDITION           0.010670\n",
            "ALIGNMENT                  0.009686\n",
            "ROADWAY_SURFACE_COND       0.009171\n",
            "AGE                        0.009143\n",
            "SEX                        0.007889\n",
            "VEHICLE_DEFECT             0.007852\n",
            "ROAD_DEFECT                0.004391\n",
            "VEHICLE_YEAR               0.002113\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# convert MOST_SEVERE_INJURY variable to NON-FATAL and FATAL\n",
        "data['MOST_SEVERE_INJURY'] = data['MOST_SEVERE_INJURY'].apply(lambda x: 'NON-FATAL' if x == 0 else 'FATAL')\n",
        "\n",
        "# calculate Cramers V for categorical features\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
        "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
        "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
        "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "# compute correlation for categorical features\n",
        "categorical_corr = pd.Series(index=categorical_features, dtype='float64')\n",
        "for feature in categorical_features:\n",
        "    categorical_corr[feature] = cramers_v(data[feature], data['MOST_SEVERE_INJURY'])\n",
        "    \n",
        "# display correlations sorted by absolute value\n",
        "print(\"Categorical Feature Correlations:\")\n",
        "print(categorical_corr.abs().sort_values(ascending=False))\n",
        "\n",
        "# checkpoint results\n",
        "categorical_corr.to_csv('./checkpoint/cramers_v.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chi-Square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Significant Features (Chi-Squared Test):\n",
            "TRAFFIC_CONTROL_DEVICE: Chi2=776.4059, P-value=3.3233e-153\n",
            "DEVICE_CONDITION: Chi2=486.5298, P-value=6.3022e-101\n",
            "WEATHER_CONDITION: Chi2=637.0532, P-value=1.6486e-129\n",
            "FIRST_CRASH_TYPE: Chi2=20467.9817, P-value=0.0000e+00\n",
            "TRAFFICWAY_TYPE: Chi2=1648.5028, P-value=0.0000e+00\n",
            "ALIGNMENT: Chi2=400.1789, P-value=2.7146e-84\n",
            "ROADWAY_SURFACE_COND: Chi2=360.3035, P-value=9.4660e-75\n",
            "ROAD_DEFECT: Chi2=87.2149, P-value=1.1469e-16\n",
            "PRIM_CONTRIBUTORY_CAUSE: Chi2=15576.0863, P-value=0.0000e+00\n",
            "SEC_CONTRIBUTORY_CAUSE: Chi2=6366.4773, P-value=0.0000e+00\n",
            "UNIT_TYPE: Chi2=9279.9496, P-value=0.0000e+00\n",
            "VEHICLE_YEAR: Chi2=19.8048, P-value=8.5765e-06\n",
            "VEHICLE_DEFECT: Chi2=275.6791, P-value=2.7082e-49\n",
            "VEHICLE_TYPE: Chi2=2121.0831, P-value=0.0000e+00\n",
            "MANEUVER: Chi2=1398.3851, P-value=4.7498e-278\n",
            "OCCUPANT_CNT: Chi2=1144.0131, P-value=1.0286e-247\n",
            "FIRST_CONTACT_POINT: Chi2=934.6493, P-value=2.6990e-202\n",
            "PERSON_TYPE: Chi2=8834.2204, P-value=0.0000e+00\n",
            "SEX: Chi2=264.1638, P-value=4.3407e-58\n",
            "AGE: Chi2=355.1475, P-value=1.1457e-76\n",
            "SAFETY_EQUIPMENT: Chi2=4509.9457, P-value=0.0000e+00\n",
            "AIRBAG_DEPLOYED: Chi2=12029.0714, P-value=0.0000e+00\n",
            "EJECTION: Chi2=11106.3368, P-value=0.0000e+00\n",
            "DRIVER_ACTION: Chi2=1566.2365, P-value=0.0000e+00\n",
            "DRIVER_VISION: Chi2=494.7520, P-value=1.9052e-97\n",
            "PHYSICAL_CONDITION: Chi2=6404.1592, P-value=0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# chi-squared test for categorical features\n",
        "chi2_results = {}\n",
        "for feature in categorical_features:\n",
        "    # calculate chi-squared statistic and p-value\n",
        "    contingency_table = pd.crosstab(data[feature], data['MOST_SEVERE_INJURY'])\n",
        "    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
        "    chi2_results[feature] = (chi2, p_value)\n",
        "    \n",
        "# filter features with p-value < 0.05\n",
        "chi2_results = {k: v for k, v in chi2_results.items() if v[1] < 0.05}\n",
        "\n",
        "# display significant features\n",
        "print(\"Significant Features (Chi-Squared Test):\")\n",
        "for feature, (chi2, p_value) in chi2_results.items():\n",
        "    print(f\"{feature}: Chi2={chi2:.4f}, P-value={p_value:.4e}\")\n",
        "\n",
        "# checkpoint results\n",
        "chi2_results_df = pd.DataFrame(chi2_results).T\n",
        "chi2_results_df.columns = ['Chi2', 'P-value']\n",
        "chi2_results_df.to_csv('./checkpoint/chi2.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Variance Inflation Factor\n",
        "- Check for multicollinearity using VIF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VIF Values:\n",
            "              feature        VIF\n",
            "0  POSTED_SPEED_LIMIT  13.621685\n",
            "1           NUM_UNITS   9.980176\n",
            "2   CRASH_DAY_OF_WEEK   4.849806\n",
            "3         CRASH_MONTH   4.535400\n",
            "4           RUSH_HOUR   1.612996\n",
            "5            DAYLIGHT   2.815999\n"
          ]
        }
      ],
      "source": [
        "# check for multicollinearity using VIF\n",
        "def calculate_vif(data):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = data.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
        "    return vif_data\n",
        "\n",
        "# vif_features = data.drop(columns=['MOST_SEVERE_INJURY']).select_dtypes(include=['float64', 'int64'])\n",
        "vif_features = data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "\n",
        "# display VIF values\n",
        "print(\"VIF Values:\")\n",
        "print(calculate_vif(vif_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VIF Results \n",
        "- Features with high VIF (>= 10) will be dropped.\n",
        "    - POSTED_SPEED_LIMIT \n",
        "    - NUM_UNITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop POSTED_SPEED_LIMIT and NUM_UNITS\n",
        "data.drop(columns=['POSTED_SPEED_LIMIT', 'NUM_UNITS'], inplace=True, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Strongest Correlations:\n",
            "POSTED_SPEED_LIMIT:\n",
            "NUM_UNITS            0.061098\n",
            "DAYLIGHT             0.037010\n",
            "CRASH_MONTH          0.010327\n",
            "CRASH_DAY_OF_WEEK    0.006360\n",
            "RUSH_HOUR            0.004616\n",
            "dtype: float64\n",
            "\n",
            "NUM_UNITS:\n",
            "POSTED_SPEED_LIMIT    0.061098\n",
            "DAYLIGHT              0.045542\n",
            "RUSH_HOUR             0.018429\n",
            "CRASH_MONTH           0.003931\n",
            "CRASH_DAY_OF_WEEK     0.002289\n",
            "dtype: float64\n",
            "\n",
            "CRASH_DAY_OF_WEEK:\n",
            "RUSH_HOUR             0.010210\n",
            "POSTED_SPEED_LIMIT    0.006360\n",
            "DAYLIGHT              0.005326\n",
            "CRASH_MONTH           0.003866\n",
            "NUM_UNITS             0.002289\n",
            "dtype: float64\n",
            "\n",
            "CRASH_MONTH:\n",
            "DAYLIGHT              0.043896\n",
            "POSTED_SPEED_LIMIT    0.010327\n",
            "NUM_UNITS             0.003931\n",
            "CRASH_DAY_OF_WEEK     0.003866\n",
            "RUSH_HOUR             0.003141\n",
            "dtype: float64\n",
            "\n",
            "RUSH_HOUR:\n",
            "DAYLIGHT              0.176756\n",
            "NUM_UNITS             0.018429\n",
            "CRASH_DAY_OF_WEEK     0.010210\n",
            "POSTED_SPEED_LIMIT    0.004616\n",
            "CRASH_MONTH           0.003141\n",
            "dtype: float64\n",
            "\n",
            "DAYLIGHT:\n",
            "RUSH_HOUR             0.176756\n",
            "NUM_UNITS             0.045542\n",
            "CRASH_MONTH           0.043896\n",
            "POSTED_SPEED_LIMIT    0.037010\n",
            "CRASH_DAY_OF_WEEK     0.005326\n",
            "dtype: float64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check the strongest correlations among the high VIF features\n",
        "strongest_correlations = {}\n",
        "for feature in vif_features.columns:\n",
        "    corr = vif_features.corrwith(vif_features[feature]).abs().sort_values(ascending=False)\n",
        "    strongest_correlations[feature] = corr[corr.index != feature].head(5)\n",
        "    \n",
        "# display strongest correlations\n",
        "print(\"Strongest Correlations:\")\n",
        "for feature, correlations in strongest_correlations.items():\n",
        "    print(f\"{feature}:\")\n",
        "    print(correlations)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Drop Features with Low Significance\n",
        "- Drop features with p-value > 0.05."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop low relevance features based on anova, chi2, spearman, and cramers_v\n",
        "# data.drop(columns=[\n",
        "    # 'VEHICLE_DEFECT', \n",
        "    # 'ALIGNMENT', \n",
        "    # 'Divided_Trafficway',\n",
        "    # 'VEHICLE_YEAR',\n",
        "    # 'Adverse_Weather',\n",
        "    # 'CRASH_DAY_BINARY',\n",
        "    # 'TRAFFICWAY_TYPE',\n",
        "    # 'DRIVER_ACTION',\n",
        "    # 'MANEUVER',\n",
        "    # 'ROADWAY_SURFACE_COND',\n",
        "    # 'LIGHTING_CONDITION',\n",
        "    # 'SEC_CAUSE_GROUP',\n",
        "    # 'ROAD_DEFECT',\n",
        "    # 'CRASH_MONTH',\n",
        "    # 'CRASH_HOUR',\n",
        "    # 'CRASH_DAY_OF_WEEK',\n",
        "    # 'WEATHER_CONDITION',\n",
        "    # 'OLD_VEHICLE',\n",
        "    # 'DRIVER_VISION'\n",
        "    # ], errors='ignore', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features after dropping: 31\n"
          ]
        }
      ],
      "source": [
        "# print the number of features before dropping\n",
        "print(\"Number of features after dropping:\", data.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checkpoint data\n",
        "data.to_csv('./checkpoint/data_post_feat_sel.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Train Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "data = pd.read_csv('./checkpoint/data_post_feat_sel.csv')\n",
        "y = data['MOST_SEVERE_INJURY']\n",
        "X = data.drop(columns=['MOST_SEVERE_INJURY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Values:\n",
            "Series([], dtype: int64)\n",
            "MOST_SEVERE_INJURY\n",
            "NON-FATAL    4205118\n",
            "FATAL           7036\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# check if there are any missing values\n",
        "missing_values = X.isnull().sum()\n",
        "print(\"Missing Values:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# align indices of X and y, drop any rows with NaNs in either, and perform a train-test split\n",
        "def train_test_split_wrapper(data, y):\n",
        "    # align indices of data and y\n",
        "    data, y = data.align(y, join=\"inner\", axis=0)\n",
        "\n",
        "    # drop rows with NaNs in either the features or the target\n",
        "    combined = pd.concat([data, y], axis=1)\n",
        "    combined = combined.dropna()\n",
        "\n",
        "    # split the cleaned data and target\n",
        "    data_cleaned = combined.iloc[:, :-1]  # All columns except the last (features)\n",
        "    y_cleaned = combined.iloc[:, -1]  # Last column (target)\n",
        "\n",
        "    # train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data_cleaned, y_cleaned, test_size=0.2, random_state=42, stratify=y_cleaned\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perform train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split_wrapper(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MOST_SEVERE_INJURY\n",
              "NON-FATAL    3364094\n",
              "FATAL           5629\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "- One-hot encoding for categorical features.\n",
        "- Standard scaling for numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(df, scaler=None, encoder=None, train=True):\n",
        "    # identify numeric & categorical features\n",
        "    numeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # scale numeric features\n",
        "    if train:\n",
        "        scaler = StandardScaler()\n",
        "        df_numeric_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_features]), \n",
        "                                         columns=numeric_features, index=df.index)\n",
        "    else:\n",
        "        if scaler is None:\n",
        "            raise ValueError(\"Scaler cannot be None when train=False\")\n",
        "        df_numeric_scaled = pd.DataFrame(scaler.transform(df[numeric_features]), \n",
        "                                         columns=numeric_features, index=df.index)\n",
        "\n",
        "    # encode categorical features\n",
        "    if train:\n",
        "        encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "        df_categorical_encoded = pd.DataFrame(encoder.fit_transform(df[categorical_features]), \n",
        "                                              columns=encoder.get_feature_names_out(categorical_features), \n",
        "                                              index=df.index)\n",
        "    else:\n",
        "        if encoder is None:\n",
        "            raise ValueError(\"Encoder cannot be None when train=False\")\n",
        "        df_categorical_encoded = pd.DataFrame(encoder.transform(df[categorical_features]), \n",
        "                                              columns=encoder.get_feature_names_out(categorical_features), \n",
        "                                              index=df.index)\n",
        "\n",
        "    # combine processed features\n",
        "    df_processed = pd.concat([df_numeric_scaled, df_categorical_encoded], axis=1)\n",
        "\n",
        "    return df_processed, scaler, encoder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensure scaler & encoder are passed for test data\n",
        "X_train_processed, scaler, encoder = preprocess_data(X_train, train=True)\n",
        "X_test_processed, _, _ = preprocess_data(X_test, scaler=scaler, encoder=encoder, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Logistic Regression Model\n",
        "\n",
        "Logistic Regression\n",
        "    Pros:\n",
        "        - Fast\n",
        "        - Interpretable\n",
        "        - Works well with binary classification\n",
        "    Cons:\n",
        "        - May not capture complex relationships\n",
        "        - May not perform well with imbalanced classes\n",
        "\n",
        "- Random Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-4 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-4 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-4 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-4 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-4 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, random_state=42, solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(max_iter=1000, random_state=42, solver='saga')"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# randomly under-sample the majority class\n",
        "rus = RandomUnderSampler(random_state=42)  \n",
        "X_train_processed_rus, y_train_rus = rus.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "# train baseline model\n",
        "model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver='saga',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train_processed_rus, y_train_rus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression Coefficients\n",
        "- Identify features with high impact on classifying Fatal/Non-Fatal injuries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features with the highest coefficients:\n",
            "                                               Feature  Coefficient\n",
            "122  PRIM_CONTRIBUTORY_CAUSE_PHYSICAL CONDITION OF ...     3.155665\n",
            "107  PRIM_CONTRIBUTORY_CAUSE_EXCEEDING AUTHORIZED S...     2.982513\n",
            "48                         FIRST_CRASH_TYPE_PEDESTRIAN     2.529505\n",
            "40                             FIRST_CRASH_TYPE_ANIMAL     2.401920\n",
            "277                           EJECTION_TOTALLY EJECTED     2.072371\n",
            "17                  TRAFFIC_CONTROL_DEVICE_SCHOOL ZONE     1.994622\n",
            "74           TRAFFICWAY_TYPE_UNKNOWN INTERSECTION TYPE     1.950277\n",
            "199                            VEHICLE_TYPE_Motorcycle     1.899112\n",
            "278                        EJECTION_TRAPPED/EXTRICATED     1.890950\n",
            "144  SEC_CONTRIBUTORY_CAUSE_EQUIPMENT - VEHICLE CON...     1.839812\n",
            "52                       FIRST_CRASH_TYPE_REAR TO SIDE     1.835741\n",
            "101  PRIM_CONTRIBUTORY_CAUSE_DISTRACTION - FROM OUT...     1.785528\n",
            "75                      TRAFFICWAY_TYPE_Y-INTERSECTION     1.776450\n",
            "71                      TRAFFICWAY_TYPE_T-INTERSECTION     1.733798\n",
            "69                                TRAFFICWAY_TYPE_RAMP     1.652816\n",
            "120  PRIM_CONTRIBUTORY_CAUSE_OPERATING VEHICLE IN E...     1.591822\n",
            "313                           PHYSICAL_CONDITION_OTHER     1.575172\n",
            "153         SEC_CONTRIBUTORY_CAUSE_IMPROPER LANE USAGE     1.558567\n",
            "104  PRIM_CONTRIBUTORY_CAUSE_DRIVING SKILLS/KNOWLED...     1.556687\n",
            "207                            VEHICLE_TYPE_UNKNOWN/NA     1.473260\n",
            "270              AIRBAG_DEPLOYED_DEPLOYED, COMBINATION     1.403889\n",
            "63                      TRAFFICWAY_TYPE_L-INTERSECTION     1.343677\n",
            "138     SEC_CONTRIBUTORY_CAUSE_DISREGARDING YIELD SIGN     1.339552\n",
            "22               DEVICE_CONDITION_FUNCTIONING PROPERLY     1.299803\n",
            "150       SEC_CONTRIBUTORY_CAUSE_FOLLOWING TOO CLOSELY     1.293462\n",
            "155  SEC_CONTRIBUTORY_CAUSE_IMPROPER TURNING/NO SIGNAL     1.272595\n",
            "7                   TRAFFIC_CONTROL_DEVICE_NO CONTROLS     1.253819\n",
            "197            VEHICLE_TYPE_MOPED OR MOTORIZED BICYCLE     1.234333\n",
            "68                         TRAFFICWAY_TYPE_PARKING LOT     1.215702\n",
            "89                              ROAD_DEFECT_RUT, HOLES     1.214324\n",
            "113           PRIM_CONTRIBUTORY_CAUSE_IMPROPER BACKING     1.214236\n",
            "116  PRIM_CONTRIBUTORY_CAUSE_IMPROPER TURNING/NO SI...     1.214067\n",
            "238                                  OCCUPANT_CNT_Solo     1.205926\n",
            "274                     AIRBAG_DEPLOYED_DID NOT DEPLOY     1.201076\n",
            "142  SEC_CONTRIBUTORY_CAUSE_DRIVING ON WRONG SIDE/W...     1.200413\n",
            "131                    PRIM_CONTRIBUTORY_CAUSE_WEATHER     1.181457\n",
            "57                    TRAFFICWAY_TYPE_CENTER TURN LANE     1.164641\n",
            "95   PRIM_CONTRIBUTORY_CAUSE_DISREGARDING OTHER TRA...     1.154113\n",
            "84                  ROADWAY_SURFACE_COND_SNOW OR SLUSH     1.141065\n",
            "143  SEC_CONTRIBUTORY_CAUSE_DRIVING SKILLS/KNOWLEDG...     1.135597\n",
            "54           FIRST_CRASH_TYPE_SIDESWIPE SAME DIRECTION     1.124305\n",
            "103  PRIM_CONTRIBUTORY_CAUSE_DRIVING ON WRONG SIDE/...     1.121768\n",
            "53       FIRST_CRASH_TYPE_SIDESWIPE OPPOSITE DIRECTION     1.101175\n",
            "178                               UNIT_TYPE_PEDESTRIAN     1.100233\n",
            "51                       FIRST_CRASH_TYPE_REAR TO REAR     1.043373\n",
            "29                      WEATHER_CONDITION_BLOWING SNOW     1.037399\n",
            "92                            ROAD_DEFECT_WORN SURFACE     1.026625\n",
            "265                  SAFETY_EQUIPMENT_SAFETY BELT USED     1.022977\n",
            "114        PRIM_CONTRIBUTORY_CAUSE_IMPROPER LANE USAGE     1.017793\n",
            "81                            ROADWAY_SURFACE_COND_ICE     1.006277\n",
            "62                            TRAFFICWAY_TYPE_FOUR WAY     1.000895\n"
          ]
        }
      ],
      "source": [
        "# get regression coefficients\n",
        "feature_importance = pd.DataFrame(\n",
        "    {\"Feature\": X_train_processed.columns, \"Coefficient\": np.abs(model.coef_[0])}\n",
        ").sort_values(\"Coefficient\", ascending=False)\n",
        "\n",
        "# features with the highest coefficients (>= 1)\n",
        "log_reg_coefficients = feature_importance[feature_importance[\"Coefficient\"] >= 1]\n",
        "\n",
        "# print features with the highest coefficients (>= 1)\n",
        "print(\"Features with the highest coefficients:\")\n",
        "print(log_reg_coefficients)\n",
        "\n",
        "# checkpoint feature importance\n",
        "log_reg_coefficients.to_csv('./checkpoint/log_reg_coefficients.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # Findings\n",
        "\n",
        "- **Driver's Physical Condition Significantly Affects Crash Severity**  \n",
        "    - PRIM_CONTRIBUTORY_CAUSE_PHYSICAL CONDITION OF DRIVER (**Coefficient:** 3.04)  \n",
        "\n",
        "    **Actionable Steps:**  \n",
        "        - Implement roadside impairment tests using AI-powered assessment tools.  \n",
        "        - Develop awareness campaigns on the risks of driving with medical impairments.  \n",
        "\n",
        "- **Speeding is a Major Predictor of Severe Crashes**  \n",
        "    - PRIM_CONTRIBUTORY_CAUSE_EXCEEDING AUTHORIZED SPEED LIMIT (**Coefficient:** 2.79)  \n",
        "\n",
        "    **Actionable Steps:**  \n",
        "        - Expand speed camera enforcement in high-risk areas.  \n",
        "        - Implement dynamic speed limits based on traffic and weather conditions.  \n",
        "        - Increase public awareness campaigns about the dangers of excessive speed.  \n",
        "\n",
        "- **Pedestrian, Cyclist, Animal Crashes Have High Severity**  \n",
        "    - FIRST_CRASH_TYPE_PEDESTRIAN (**Coefficient:** 2.55)  \n",
        "    - FIRST_CRASH_TYPE_PEDALCYCLIST (**Coefficient:** 1.39)  \n",
        "    - FIRST_CRASH_TYPE_ANIMAL (**Coefficient:** 2.428)\n",
        "    \n",
        "\n",
        "    **Actionable Steps:**  \n",
        "        - Improve car traffic isolation from bike lanes and pedestrian crossings.\n",
        "        - Implement wildlife detection systems to alert drivers of animal crossings.\n",
        "        - Increase visibility and signage at pedestrian and cyclist crossings.\n",
        "        - Implement wildlife crossing deterrents (eg. fencing) in high-risk areas.\n",
        "\n",
        "- **Dangerous Roadway Conditions Increase Crash Severity**  \n",
        "    - ROAD_DEFECT_WORN SURFACE (**Coefficient:** 0.97)  \n",
        "    - ROAD_DEFECT_RUT, HOLES (**Coefficient:** 0.51)  \n",
        "    - ROADWAY_SURFACE_COND_SNOW OR SLUSH (**Coefficient:** 0.61)  \n",
        "\n",
        "    **Actionable Steps:**  \n",
        "        - Prioritize road maintenance funding for fixing potholes and worn surfaces.  \n",
        "        - Implement real-time road hazard alerts using IoT and traffic apps.  \n",
        "        - Expand pre-winter road treatment programs to reduce ice-related crashes.  \n",
        "\n",
        "- **High-Risk Intersection Designs Need Attention**  \n",
        "    - TRAFFICWAY_TYPE_Y-INTERSECTION (**Coefficient:** 1.28)  \n",
        "    - TRAFFICWAY_TYPE_T-INTERSECTION (**Coefficient:** 1.50)  \n",
        "    - TRAFFICWAY_TYPE_UNKNOWN INTERSECTION TYPE (**Coefficient:** 1.93)\n",
        "    - TRAFFICWAY_TYPE_L-INTERSECTION (**Coefficient:** 1.34) \n",
        "\n",
        "\n",
        "    **Actionable Steps:**  \n",
        "        - Improve signage and lane markings for better driver guidance.  \n",
        "        - Deploy collision-prevention warning systems at intersections.  \n",
        "        - Implement red-light cameras at high-risk intersections.\n",
        "        - Implement traffic slowing measures at non-standard or high-risk intersections.\n",
        "\n",
        "- **Distracted and Reckless Driving Remains a Major Threat**  \n",
        "    - PRIM_CONTRIBUTORY_CAUSE_DISTRACTION - FROM OUTSIDE VEHICLE (**Coefficient:** 1.26)  \n",
        "    - PRIM_CONTRIBUTORY_CAUSE_OPERATING VEHICLE IN ERRATIC, RECKLESS, CARELESS, NEGLIGENT OR AGGRESSIVE MANNER (**Coefficient:** 1.66)  \n",
        "    - PRIM_CONTRIBUTORY_CAUSE_CELL PHONE USE OTHER THAN TEXTING (**Coefficient:** 0.42)  \n",
        "\n",
        "    **Actionable Steps:**  \n",
        "        - Implement stricter penalties for distracted and reckless driving.  \n",
        "        - Launch public awareness campaigns on the dangers of distracted driving.  \n",
        "        - Install CCTV cameras at high-risk intersections to monitor driver behavior and issue fines.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X, y, X_test, y_test, output_path='./checkpoint/evaluation_metrics.json', cv_folds=5):\n",
        "    # classification report\n",
        "    print(\"Classification Report:\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "\n",
        "    # AUC-ROC\n",
        "    roc_auc = roc_auc_score(pd.get_dummies(y_test).values, y_pred_proba, multi_class=\"ovr\")\n",
        "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "    # confusion matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    # cross-validation\n",
        "    # print(\"\\nRunning Cross-Validation...\")\n",
        "    # cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    # cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "    # print cross validation scores\n",
        "    # print(\"Cross-Validation Scores:\")\n",
        "    # print(cv_scores)\n",
        "\n",
        "    # print cross-validation accuracy\n",
        "    # print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "    # save evaluation metrics\n",
        "    evaluation_metrics = {\n",
        "        'Classification Report': classification_report(y_test, y_pred, output_dict=True),\n",
        "        'AUC-ROC': roc_auc,\n",
        "        'Confusion Matrix': cm.tolist(),\n",
        "        #'Cross-Validation Accuracy': {\n",
        "        #    'mean': cv_scores.mean(),\n",
        "        #    'std_dev': cv_scores.std(),\n",
        "        #    'scores': cv_scores.tolist()\n",
        "        #}\n",
        "    }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(evaluation_metrics, f, indent=4)\n",
        "\n",
        "    return evaluation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FATAL       0.01      0.88      0.02      1407\n",
            "   NON-FATAL       1.00      0.88      0.93    841024\n",
            "\n",
            "    accuracy                           0.88    842431\n",
            "   macro avg       0.51      0.88      0.48    842431\n",
            "weighted avg       1.00      0.88      0.93    842431\n",
            "\n",
            "AUC-ROC: 0.9388\n",
            "Confusion Matrix:\n",
            "[[  1234    173]\n",
            " [103429 737595]]\n"
          ]
        }
      ],
      "source": [
        "# evaluate the baseline model\n",
        "evaluation_metrics = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_baseline.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Evaluation\n",
        "- Struggles with class imbalance, poor precision and recall for fatal class.\n",
        "- Likely overfitting and will be unable to generalize to new data.\n",
        "- AUC-ROC score indicates strong discriminatory power.\n",
        "    - With low precision suggests that threshold tuning may be needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Tuning\n",
        "- Threshold tuning to improve precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FATAL       0.01      0.97      0.01      1407\n",
            "   NON-FATAL       1.00      0.72      0.84    841024\n",
            "\n",
            "    accuracy                           0.72    842431\n",
            "   macro avg       0.50      0.84      0.42    842431\n",
            "weighted avg       1.00      0.72      0.84    842431\n",
            "\n",
            "AUC-ROC: 0.9389\n",
            "Confusion Matrix:\n",
            "[[  1363     44]\n",
            " [235444 605580]]\n"
          ]
        }
      ],
      "source": [
        "# train baseline model\n",
        "model = LogisticRegression(\n",
        "    # class_weight={'NON-FATAL': 1, 'FATAL': 10},\n",
        "    max_iter=500,\n",
        "    solver='saga',\n",
        "    class_weight={'NON-FATAL': 1, 'FATAL': 5},\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train_processed_rus, y_train_rus)\n",
        "\n",
        "# evaluate the baseline model\n",
        "evaluation_metrics = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_baseline_weighted.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuned Logistic Regression Evaluation\n",
        "- Better:\n",
        "    - captures as many fatal crashes as possible (high recall)\n",
        "- Worse:\n",
        "    - fewer false positives and better overall accuracy\n",
        "    - more false negatives and lower precision\n",
        "\n",
        "Ultimately, logistic regression is not the best model for this task due to class imbalance and the need for better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Decision Tree\n",
        "- Use a decision tree classifier\n",
        "    Pros:\n",
        "        - Handles non-linear relationships capturing interactions and feature importance.\n",
        "        - Works better with imbalanced data.\n",
        "        - More flexible than logistic regression.\n",
        "    Cons:\n",
        "        - May overfit the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FATAL       0.01      0.73      0.01      1407\n",
            "   NON-FATAL       1.00      0.83      0.91    841024\n",
            "\n",
            "    accuracy                           0.83    842431\n",
            "   macro avg       0.50      0.78      0.46    842431\n",
            "weighted avg       1.00      0.83      0.91    842431\n",
            "\n",
            "AUC-ROC: 0.8126\n",
            "Confusion Matrix:\n",
            "[[  1024    383]\n",
            " [143369 697655]]\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "model = DecisionTreeClassifier(\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    max_depth=5\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train_processed_rus, y_train_rus)\n",
        "\n",
        "# evaluate the model\n",
        "evaluation_metrics = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_decision_tree.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Decision Tree Evaluation\n",
        "\n",
        "- Worse:\n",
        "    - Misses more actual fatal crashes, making it less reliable for high-risk events\n",
        "    - Decision Tree has higher false negatives (383 missed FATAL crashes)\n",
        "    - AUC-ROC is significantly lower, indicating weaker discriminatory power\n",
        "\n",
        "The model is largely ineffective at predicting fatal crashes and has not improved significantly over the logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tuned Decision Tree\n",
        "- Tune the decision tree model to improve performance.\n",
        "    - Increase max depth to capture more complex relationships.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FATAL       0.01      0.87      0.03      1407\n",
            "   NON-FATAL       1.00      0.89      0.94    841024\n",
            "\n",
            "    accuracy                           0.89    842431\n",
            "   macro avg       0.51      0.88      0.48    842431\n",
            "weighted avg       1.00      0.89      0.94    842431\n",
            "\n",
            "AUC-ROC: 0.9017\n",
            "Confusion Matrix:\n",
            "[[  1219    188]\n",
            " [ 94275 746749]]\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "model = DecisionTreeClassifier(\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    max_depth=20, \n",
        "    min_samples_split=10, \n",
        "    min_samples_leaf=2\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train_processed_rus, y_train_rus)\n",
        "\n",
        "# evaluate the model\n",
        "evaluation_metrics = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_decision_tree_tuned.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Results\n",
        "- Decision Tree Classifier performs better than the baseline model.\n",
        "    - ROC-AUC: Logistic Regression is slightly better at distinguishing between classes.\n",
        "    - Weighted Average F1 Score: Decision Tree Classifier is better at predicting injury severity.\n",
        "    - Accuracy: Decision Tree is overall better at predicting injury severity.\n",
        "- Logistic Regression suffers from class imbalance and is not able to predict injury severity effectively.\n",
        "\n",
        "Logistic Regression:\n",
        "- It heavily predicts the majority class: \n",
        "    - Fatal injuries (FATAL): Poor recall (69%), likely due to underrepresentation.\n",
        "    - Struggles to separate \"Injury\" classes (e.g., Incapacitating vs. Non-incapacitating).\n",
        "Decision Tree:\n",
        "- Better balance in predictions, but slightly worse AUC-ROC.\n",
        "    - Higher recall for all injury types, meaning it captures more true injuries.\n",
        "    - Slightly more false positives in injury-related classes, which could be tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tuning\n",
        "- Hyperparameter tuning for Decision Tree Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FATAL       0.01      0.87      0.03      1407\n",
            "   NON-FATAL       1.00      0.89      0.94    841024\n",
            "\n",
            "    accuracy                           0.89    842431\n",
            "   macro avg       0.51      0.88      0.48    842431\n",
            "weighted avg       1.00      0.89      0.94    842431\n",
            "\n",
            "AUC-ROC: 0.9017\n",
            "Confusion Matrix:\n",
            "[[  1219    188]\n",
            " [ 94275 746749]]\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "model = DecisionTreeClassifier(\n",
        "    # class_weight={'NON-FATAL': 1, 'FATAL': 10},\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    max_depth=20, \n",
        "    min_samples_split=10, \n",
        "    min_samples_leaf=2\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train_processed_rus, y_train_rus)\n",
        "\n",
        "# evaluate the model\n",
        "evaluation_metrics = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, output_path='./checkpoint/evaluation_metrics_decision_tree_tuned.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features with the highest importance:\n",
            "                                           Feature  Importance\n",
            "270          AIRBAG_DEPLOYED_DEPLOYED, COMBINATION    0.153815\n",
            "48                     FIRST_CRASH_TYPE_PEDESTRIAN    0.127106\n",
            "271                AIRBAG_DEPLOYED_DEPLOYED, FRONT    0.070910\n",
            "242                             PERSON_TYPE_DRIVER    0.048126\n",
            "265              SAFETY_EQUIPMENT_SAFETY BELT USED    0.042292\n",
            "..                                             ...         ...\n",
            "38                          WEATHER_CONDITION_SNOW    0.000102\n",
            "136  SEC_CONTRIBUTORY_CAUSE_DISREGARDING STOP SIGN    0.000094\n",
            "288                 DRIVER_ACTION_IMPROPER PASSING    0.000072\n",
            "261                   SAFETY_EQUIPMENT_HELMET USED    0.000065\n",
            "82                      ROADWAY_SURFACE_COND_OTHER    0.000044\n",
            "\n",
            "[148 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# get model coefficients\n",
        "feature_importance = pd.DataFrame(\n",
        "    {\"Feature\": X_train_processed.columns, \"Importance\": model.feature_importances_}\n",
        ").sort_values(\"Importance\", ascending=False)\n",
        "\n",
        "# features with the highest importance\n",
        "decision_tree_importance = feature_importance[feature_importance[\"Importance\"] > 0]\n",
        "\n",
        "# print features with the highest importance\n",
        "print(\"Features with the highest importance:\")\n",
        "print(decision_tree_importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tuned Decision Tree Evaluation\n",
        "- Better:\n",
        "    - Offers a better balance between precision and recall for all classes.\n",
        "- Worse:\n",
        "    - AUC-ROC is significantly lower than regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "- Class imbalance continues to be a challenge in predicting the minority class (FATAL injuries). \n",
        "- Decision Tree Classifier offers a better balance between precision and recall for all classes.\n",
        "- Logistic Regression struggles with class imbalance and is not able to predict injury severity effectively.\n",
        "    - But it has a higher AUC-ROC score, indicating better discriminatory power.\n",
        "\n",
        "Choosing a model depends on the stakeholder's priorities:\n",
        "- Capturing the most fatal crashes -> the Baseline Weighted model. \n",
        "- Balancing accuracy and precision -> the Baseline model. \n",
        "- Minimizing false positives and maximizing recall -> the Tuned Decision Tree model. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
